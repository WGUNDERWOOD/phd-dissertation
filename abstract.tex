%! TeX root = dissertation.tex

% 350 words max

% why np is important
Nonparametric methodology is central to modern statistics, enabling
data analysis with minimal assumptions in a wide range of scenarios.
While contemporary procedures such as random forests and kernel
methods are popular due to their performance, flexibility and efficiency, their
statistical properties are often less well understood.
The availability of inferential techniques is especially vital,
allowing researchers to quantify the uncertainty in their models.
We analyze procedures for robust and practical statistical estimation and
inference in some modern nonparametric settings involving complex estimators
and nontraditional data.

% mondrian
We begin in the regression setting where we study the Mondrian random forest, a
random forest variant in which the partitions come from a Mondrian process. We
present a comprehensive analysis of the statistical properties of Mondrian
random forests, including a central limit theorem for the estimated regression
function and a characterization of the bias. We show how to conduct feasible
and valid nonparametric inference by constructing confidence intervals. We
further provide a debiasing procedure which enables minimax-optimal estimation
rates for smooth function classes in arbitrary dimension.

% kernel
Next, we turn our attention to nonparametric estimation with dependent dyadic
network data. We investigate the problem of estimating a dyadic Lebesgue
density function using a kernel-based density estimator, and present results
for minimax-optimal estimation and uniform inference. We provide uniform
confidence bands and counterfactual estimation procedures which are adaptive to
dyadic degeneracy, and give illustrations with both simulated and real-world
data. Our technical contributions may be of independent interest.

% yurinskii
Finally we develop some new probabilistic results with applications to
nonparametric inference. Coupling has become a popular approach for
distributional analysis in recent years, and Yurinskii's coupling stands out
for its wide applicability and explicit formulation. We present a
generalization of Yurinskii's coupling, treating approximate martingale data
under weaker conditions than previously imposed. We allow for Gaussian mixture
coupling distributions, and a third-order method gives faster rates in certain
situations. We showcase our results with applications to factor models and
martingale empirical processes, as well as nonparametric partitioning-based and
local polynomial regression procedures.
