%! TeX root = dissertation.tex

\chapter{Appendix to Inference with Mondrian Random Forests}

\section{Proofs and technical results}%
\label{sec:mondrian_proofs}

In this section we present the full proofs of all our results,
and also state some useful technical preliminary and
intermediate lemmas.
See Section~\ref{sec:overview_proofs} in the main paper
for an overview of the main proof strategies and a discussion of
the challenges involved.
We use the following simplified notation for convenience,
whenever it is appropriate.
We write $\I_{i b}(x) = \I \left\{ X_i \in T_b(x) \right\}$
and $N_b(x) = \sum_{i=1}^{n} \I_{i b}(x)$,
as well as $\I_b(x) = \I \left\{ N_b(x) \geq 1 \right\}$.

\subsection{Preliminary lemmas}

We begin by bounding the maximum size of any cell
in a Mondrian forest containing $x$.
This result is used regularly throughout many of our other proofs,
and captures the ``localizing'' behavior of the Mondrian random
forest estimator, showing that Mondrian cells have side lengths
at most on the order of $1/\lambda$.

\begin{lemma}[Upper bound on the largest cell in a Mondrian forest]%
  \label{lem:largest_cell}
  %
  Let $T_1, \ldots, T_b \sim \cM\big([0,1]^d, \lambda\big)$
  and take $x \in (0,1)^d$. Then for all $t > 0$
  %
  \begin{align*}
    \P \left(
      \max_{1 \leq b \leq B}
      \max_{1 \leq j \leq d}
      |T_b(x)_j|
      \geq \frac{t}{\lambda}
    \right)
    &\leq
    2dB e^{-t/2}.
  \end{align*}

\end{lemma}

\begin{proof}[Lemma~\ref{lem:largest_cell}]
  We use the explicit distribution of the shape of Mondrian cells
  given by \citet[Proposition~1]{mourtada2020minimax}.
  In particular, we have
  $|T_b(x)_j| = \left( \frac{E_{bj1}}{\lambda} \wedge x_j \right)
  + \left( \frac{E_{bj2}}{\lambda} \wedge (1-x_j) \right)$
  where $E_{bj1}$ and $E_{bj2}$
  are independent $\Exp(1)$ random variables for
  $1 \leq b \leq B$ and $1 \leq j \leq d$.
  Thus $|T_b(x)_j| \leq \frac{E_{bj1} + E_{bj2}}{\lambda}$
  and so by a union bound
  %
  \begin{align*}
    \P \left(
      \max_{1 \leq b \leq B}
      \max_{1 \leq j \leq d}
      |T_b(x)_j|
      \geq \frac{t}{\lambda}
    \right)
    &\leq
    \P \left(
      \max_{1 \leq b \leq B}
      \max_{1 \leq j \leq d}
      (E_{bj1} \vee E_{bj2})
      \geq \frac{t}{2}
    \right) \\
    &\leq
    2dB\,
    \P \left(
      E_{bj1}
      \geq \frac{t}{2}
    \right)
    \leq
    2dB e^{-t/2}.
  \end{align*}
  %
\end{proof}

The next result is another ``localization'' result, this time
showing that the union over the forest
of the cells $T_b(x)$ containing $x$ do not contain ``too many''
samples $X_i$.
In other words, the Mondrian random forest estimator fitted at $x$
should only depend on $n/\lambda^d$ (the effective sample size)
data points up to logarithmic terms.

\begin{lemma}[Upper bound on the number of active data points]%
  \label{lem:active_data}
  Suppose Assumptions~\ref{ass:mondrian_data} and \ref{ass:estimator} hold
  and define
  $N_{\cup}(x) =
  \sum_{i=1}^{n} \I \left\{ X_i \in \bigcup_{b=1}^{B} T_b(x) \right\}$.
  Then for $t > 0$ and sufficiently large $n$,
  with $\|f\|_\infty = \sup_{x \in [0,1]^d} f(x)$,
  %
  \begin{align*}
    \P \left( N_{\cup}(x) > t^{d+1}
      \frac{n}{\lambda^d}
      \|f\|_\infty
    \right)
    &\leq
    4 d B e^{-t/4}.
  \end{align*}
\end{lemma}

\begin{proof}[Lemma~\ref{lem:active_data}]

  Note
  $N_\cup(x) \sim
  \Bin\left(n, \int_{\bigcup_{b=1}^{B} T_b(x)} f(s) \diff s \right)
  \leq \Bin\left(n, 2^d \max_{1 \leq b \leq B} \max_{1 \leq j \leq d}
  |T_b(x)_j|^d \|f\|_\infty \right)$
  conditionally on $\bT$.
  If $N \sim \Bin(n,p)$ then, by Bernstein's inequality,
  $\P\left( N \geq (1 + t) n p\right)
  \leq \exp\left(-\frac{t^2 n^2 p^2 / 2}{n p(1-p) + t n p / 3}\right)
  \leq \exp\left(-\frac{3t^2 n p}{6 + 2t}\right)$.
  Thus for $t \geq 2$,
  %
  \begin{align*}
    \P \left( N_{\cup}(x) > (1+t) n \frac{2^d t^d}{\lambda^d}
      \|f\|_\infty
      \Bigm| \max_{1 \leq b \leq B} \max_{1 \leq j \leq d}
      |T_j(x)| \leq \frac{t}{\lambda}
    \right)
    &\leq
    \exp\left(- \frac{2^d t^{d} n}{\lambda^d}\right).
  \end{align*}
  %
  By Lemma~\ref{lem:largest_cell},
  $\P \left( \max_{1 \leq b \leq B} \max_{1 \leq j \leq d}
  |T_j(x)| > \frac{t}{\lambda} \right)
  \leq 2 d B e^{-t/2}$.
  Hence
  %
  \begin{align*}
    &\P \left( N_{\cup}(x) > 2^{d+1} t^{d+1} \frac{n}{\lambda^d}
      \|f\|_\infty
    \right) \\
    &\quad\leq
    \P \left( N_{\cup}(x) > 2 t n \frac{2^d t^d}{\lambda^d}
      \|f\|_\infty
      \Bigm| \max_{1 \leq b \leq B} \max_{1 \leq j \leq d}
      |T_j(x)| \leq \frac{t}{\lambda}
    \right)
    + \P \left( \max_{1 \leq b \leq B} \max_{1 \leq j \leq d}
      |T_j(x)| > \frac{t}{\lambda}
    \right) \\
    &\quad\leq
    \exp\left(- \frac{2^d t^{d} n}{\lambda^d}\right)
    + 2 d B e^{-t/2}.
  \end{align*}
  %
  Replacing $t$ by $t/2$
  gives that for sufficiently large $n$ such that
  $n / \lambda^d \geq 1$,
  %
  \begin{align*}
    \P \left( N_{\cup}(x) > t^{d+1}
      \frac{n}{\lambda^d}
      \|f\|_\infty
    \right)
    &\leq
    4 d B e^{-t/4}.
  \end{align*}
\end{proof}

Next we give a series of results culminating in a
generalized moment bound for the denominator appearing
in the Mondrian random forest estimator.
We begin by providing a moment bound for the truncated inverse binomial
distribution, which will be useful for controlling
$\frac{\I_b(x)}{N_b(x)} \leq 1 \wedge \frac{1}{N_b(x)}$
because conditional on $T_b$ we have
$N_b(x) \sim \Bin \left( n, \int_{T_b(x)} f(s) \diff s \right)$.
Our constants could be significantly suboptimal but they are sufficient
for our applications.

\begin{lemma}[An inverse moment bound for the binomial distribution]%
  \label{lem:binomial_bound}
  For $n \geq 1$ and $p \in [0,1]$,
  let $N \sim \Bin(n, p)$ and $a_1, \ldots, a_k \geq 0$.
  Then
  %
  \begin{align*}
    \E\left[
      \prod_{j=1}^k
      \left(
        1 \wedge
        \frac{1}{N + a_j}
      \right)
    \right]
    &\leq
    (9k)^k
    \prod_{j=1}^k
    \left(
      1 \wedge
      \frac{1}{n p + a_j}
    \right).
  \end{align*}
\end{lemma}

\begin{proof}[Lemma~\ref{lem:binomial_bound}]
  By Bernstein's inequality,
  $\P\left( N \leq n p - t \right)
  \leq \exp\left(-\frac{t^2/2}{n p(1-p) + t/3}\right)
  \leq \exp\left(-\frac{3t^2}{6n p + 2t}\right)$.
  Therefore we have
  $\P\left( N \leq n p/4 \right)
  \leq \exp\left(-\frac{27 n^2 p^2 / 16}{6n p + 3 n p / 2}\right)
  = e^{-9 n p / 40}$.
  Partitioning by this event gives
  %
  \begin{align*}
    \E\left[
      \prod_{j=1}^k
      \left(
        1 \wedge
        \frac{1}{N + a_j}
      \right)
    \right]
    &\leq
    e^{-9 n p / 40}
    \prod_{j=1}^k
    \frac{1}{1 \vee a_j}
    + \prod_{j=1}^k
    \frac{1}{1 \vee (\frac{n p}{4} + a_j)} \\
    &\leq
    \prod_{j=1}^k
    \frac{1}{\frac{9 n p}{40k} + (1 \vee a_j)}
    + \prod_{j=1}^k
    \frac{1}{1 \vee (\frac{n p}{4} + a_j)} \\
    &\leq
    \prod_{j=1}^k
    \frac{1}{1 \vee \left(\frac{9 n p}{40k} + a_j\right)}
    + \prod_{j=1}^k
    \frac{1}{1 \vee (\frac{n p}{4} + a_j)} \\
    &\leq
    2 \prod_{j=1}^k
    \frac{1}{1 \vee \left(\frac{9 n p}{40k} + a_j\right)} \\
    &\leq
    2 \prod_{j=1}^k
    \frac{40k/9}{1 \vee \left(n p + a_j\right)} \\
    &\leq
    (9k)^k
    \prod_{j=1}^k
    \left(
      1 \wedge
      \frac{1}{n p + a_j}
    \right).
  \end{align*}
\end{proof}

Our next result is probably the most technically involved in the paper,
allowing one to bound moments of
(products of) $\frac{\I_b(x)}{N_b(x)}$ by the corresponding moments of
(products of) $\frac{1}{n |T_b(x)|}$, again based on the heuristic
that $N_b(x)$ is conditionally binomial so concentrates around
its conditional expectation
$n \int_{T_b(x)} f(x) \diff s \asymp n |T_b(x)|$.
By independence of the trees,
the latter expected products then factorize
since the dependence on the data $X_i$ has been eliminated.
The proof is complicated, and relies on the following induction procedure.
First we consider the common refinement consisting of the
subcells $\cR$ generated by all possible intersections
of $T_b(x)$ over the selected trees
(say $T_{b}(x), T_{b'}(x), T_{b''}(x)$
though there could be arbitrarily many).
Note that $N_b(x)$ is the sum of the number of
samples $X_i$ in each such subcell in $\cR$.
We then apply Lemma~\ref{lem:binomial_bound} repeatedly
to each subcell in $\cR$ in turn, replacing
the number of samples $X_i$ in that subcell with its volume
multiplied by $n$, and controlling the error incurred at each step.
We record the subcells which have been ``checked'' in this manner
using the class $\cD \subseteq \cR$ and proceed by finite induction,
beginning with $\cD = \emptyset$ and ending at $\cD = \cR$.

\begin{lemma}[Generalized moment bound for
  Mondrian random forest denominators]%
  \label{lem:moment_denominator}

  Suppose Assumptions~\ref{ass:mondrian_data}
  and \ref{ass:estimator} hold.
  Let $T_b \sim \cM\big([0,1]^d, \lambda\big)$
  be independent and $k_b \geq 1$ for $1 \leq b \leq B_0$.
  Then with $k = \sum_{b=1}^{B_0} k_b$,
  for sufficiently large $n$,
  %
  \begin{align*}
    \E\left[
      \prod_{b=1}^{B_0}
      \frac{\I_b(x)}{N_b(x)^{k_b}}
    \right]
    &\leq
    \left( \frac{36k}{\inf_{x \in [0,1]^d} f(x)} \right)^{2^{B_0} k}
    \prod_{b=1}^{B_0}
    \E \left[
      1 \wedge
      \frac{1}{(n |T_b(x)|)^{k_b}}
    \right].
  \end{align*}
\end{lemma}

\begin{proof}[Lemma~\ref{lem:moment_denominator}]

  Define the common refinement of
  $\left\{ T_b(x) : 1 \leq b \leq {B_0} \right\}$ as
  the class of sets
  %
  \begin{align*}
    \cR
    &= \left\{ \bigcap_{b=1}^{{B_0}} D_b :
      D_b \in
      \big\{ T_b(x), T_b(x)^\c \big\}
    \right\}
    \bigsetminus
    \left\{
      \emptyset,\,
      \bigcap_{b=1}^{{B_0}}
      T_b(x)^\c
    \right\}
  \end{align*}
  %
  and let $\cD \subset \cR$.
  We will proceed by induction on the elements of $\cD$,
  which represents the subcells we have checked,
  starting from $\cD = \emptyset$ and finishing at $\cD = \cR$.
  For $D \in \cR$ let
  $\cA(D) = \left\{ 1 \leq b \leq {B_0} : D \subseteq T_b(x) \right\}$
  be the indices of the trees which are active on subcell $D$,
  and for $1 \leq b \leq {B_0}$ let
  $\cA(b) = \left\{ D \in \cR : D \subseteq T_b(x) \right\}$
  be the subcells which are contained in $T_b(x)$,
  so that $b \in \cA(D) \iff D \in \cA(b)$.
  For a subcell $D \in \cR$,
  write $N_b(D) = \sum_{i=1}^{n} \I \left\{ X_i \in D \right\}$
  so that $N_b(x) = \sum_{D \in \cA(b)} N_b(D)$.
  Note that for any $D \in \cR \setminus \cD$,
  %
  \begin{align*}
    &\E \left[
      \prod_{b=1}^{B_0}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus \cD}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap \cD}
          |D'|
        \right)^{k_b}
      }
    \right] \\
    &\quad=
    \E \left[
      \prod_{b \notin \cA(D)}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus \cD}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap \cD}
          |D'|
        \right)^{k_b}
      } \right. \\
      &\left.
      \qquad
      \times\,\E\left[
        \prod_{b \in \cA(D)}
        \frac{1}{
          1 \vee \left(
            \sum_{D' \in \cA(b) \setminus \cD}
            N_b(D')
            + n \sum_{D' \in \cA(b) \cap \cD}
            |D'|
          \right)^{k_b}
        } \right.\right. \\
        &\left.\left.
        \quad\qquad\qquad\biggm|
        \bT,
        N_b(D') : D' \in \cR
        \setminus
        (\cD \cup \{D\})
      \right]
    \right].
  \end{align*}
  %
  Now the inner conditional expectation is over $N_b(D)$ only.
  Since $f$ is bounded away from zero,
  %
  \begin{align*}
    N_b(D)
    &\sim \Bin\left(
      n - \sum_{D' \in \cR \setminus (\cD \cup \{D\})} N_b(D'), \
      \frac{\int_{D} f(s) \diff s}
      {1 - \int_{\bigcup \left( \cR \setminus \cD \right) \setminus D}
      f(s) \diff s}
    \right) \\
    &\geq \Bin\left(
      n - \sum_{D' \in \cR \setminus (\cD \cup \{D\})} N_b(D'), \
      |D| \inf_{x \in [0,1]^d} f(x)
    \right)
  \end{align*}
  %
  conditional on $\bT$ and
  $N_b(D') : D' \in \cR \setminus (\cD \cup \{D\})$.
  Further, for sufficiently large $t$
  by Lemma~\ref{lem:active_data}
  %
  \begin{align*}
    \P \left(
      \sum_{D' \in \cR \setminus (\cD \cup \{D\})} N_b(D')
    > t^{d+1} \frac{n}{\lambda^d} \|f\|_\infty \right)
    &\leq
    \P \left( N_{\cup}(x) > t^{d+1}
      \frac{n}{\lambda^d}
      \|f\|_\infty
    \right)
    \leq
    4 d B_0 e^{-t/4}.
  \end{align*}
  %
  Thus
  $N_b(D) \geq \Bin(n/2, |D| \inf_x f(x))$
  conditional on
  $\left\{ \bT, N_b(D') : D' \in \cR \setminus (\cD \cup \{D\}) \right\}$
  with probability at least
  $1 - 4 d B_0 e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}}$.
  So by Lemma~\ref{lem:binomial_bound},
  %
  \begin{align*}
    &\E \Bigg[
      \prod_{b \in \cA(D)}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus \cD}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap \cD}
          |D'|
        \right)^{k_b}
      } \\
      &\qquad
      \biggm|
      \bT,
      N_b(D') : D' \in \cR \setminus (\cD \cup \{D\})
    \Bigg] \\
    &\quad\leq
    \E \! \left[
      \prod_{b \in \cA(D)}
      \frac{(9k)^{k_b}}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus (\cD \cup \{D\})}
          N_b(D')
          + n |D| \inf_x f(x) / 2
          + n \sum_{D' \in \cA(b) \cap \cD}
          |D'|
      \right)^{k_b}}
    \right] \\
    &\qquad+
    4 d B_0 e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}} \\
    &\quad\leq
    \left( \frac{18k}{\inf_x f(x)} \right)^k
    \! \E \! \left[
      \prod_{b \in \cA(D)}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus (\cD \cup \{D\})}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap (\cD \cup \{D\})}
          |D'|
      \right)^{k_b}}
    \right] \\
    &\qquad+
    4 d B_0 e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}}.
  \end{align*}
  %
  Therefore plugging this back into the marginal expectation yields
  %
  \begin{align*}
    &\E\left[
      \prod_{b=1}^{B_0}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus \cD}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap \cD}
          |D'|
        \right)^{k_b}
      }
    \right] \\
    &\quad\leq
    \left( \frac{18k}{\inf_x f(x)} \right)^k
    \E \left[
      \prod_{b=1}^{B_0}
      \frac{1}{
        1 \vee \left(
          \sum_{D' \in \cA(b) \setminus (\cD \cup \{D\})}
          N_b(D')
          + n \sum_{D' \in \cA(b) \cap (\cD \cup \{D\})}
          |D'|
      \right)^{k_b}}
    \right] \\
    &\qquad+
    4 d B_0 e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}}.
  \end{align*}
  %
  Now we apply induction,
  starting with $\cD = \emptyset$ and
  adding $D \in \cR \setminus \cD$ to $\cD$ until
  $\cD = \cR$.
  This takes at most $|\cR| \leq 2^{B_0}$ steps and yields
  %
  \begin{align*}
    \E\left[
      \prod_{b=1}^{B_0}
      \frac{\I_b(x)}{N_b(x)^{k_b}}
    \right]
    &\leq
    \E\left[
      \prod_{b=1}^{B_0}
      \frac{1}{1 \vee N_b(x)^{k_b}}
    \right]
    =
    \E\left[
      \prod_{b=1}^{B_0}
      \frac{1}{1 \vee \left( \sum_{D \in \cA(b)} N_b(D) \right)^{k_b}}
    \right]
    \leq \cdots \\
    &\leq
    \left( \frac{18k}{\inf_x f(x)} \right)^{2^{B_0} k}
    \left(
      \prod_{b=1}^{B_0}
      \,\E \left[
        \frac{1}{1 \vee (n |T_b(x)|)^{k_b}}
      \right]
      + 4 d B_0 2^{B_0} e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}}
    \right),
  \end{align*}
  %
  where the expectation factorizes due to independence of $T_b(x)$.
  The last step is to remove the trailing exponential term.
  To do this, note that by Jensen's inequality,
  %
  \begin{align*}
    \prod_{b=1}^{B_0}
    \,\E \left[
      \frac{1}{1 \vee (n |T_b(x)|)^{k_b}}
    \right]
    &\geq
    \prod_{b=1}^{B_0}
    \frac{1}
    {\E \left[ 1 \vee (n |T_b(x)|)^{k_b} \right]}
    \geq
    \prod_{b=1}^{B_0}
    \frac{1}{n^{k_b}}
    = n^{-k}
    \geq
    4 d B_0 2^{B_0} e^{\frac{-\sqrt \lambda}{8 \|f\|_\infty}}
  \end{align*}
  %
  for sufficiently large $n$
  because $B_0$, $d$, and $k$ are fixed while
  $\log \lambda \gtrsim \log n$.
\end{proof}

Now that moments of (products of) $\frac{\I_b(x)}{N_b(x)}$
have been bounded by moments of
(products of) $\frac{1}{n |T_b(x)|}$, we establish further
explicit bounds for these in the next result.
Note that the problem has been reduced to determining
properties of Mondrian cells, so once again we return to the
exact cell shape distribution given by \citet{mourtada2020minimax},
and evaluate the appropriate expectations by integration.
Note that the truncation by taking the minimum with one inside the expectation
is essential here, as otherwise second moment of the inverse Mondrian cell
volume is not even finite. As such, there is a ``penalty'' of $\log n$
when bounding truncated second moments,
and the upper bound for the $k$th moment is significantly
larger than the naive assumption of $(\lambda^d / n)^k$
whenever $k \geq 3$.
This ``small cell'' phenomenon in which the inverse volumes of Mondrian cells
have heavy tails is a recurring challenge in our analysis.

\begin{lemma}[Inverse moments of the volume of a Mondrian cell]%
  \label{lem:moment_cell}

  Suppose Assumption~\ref{ass:estimator} holds
  and let $T \sim \cM\big([0,1]^d, \lambda\big)$.
  Then for sufficiently large $n$,
  %
  \begin{align*}
    \E\left[
      1 \wedge
      \frac{1}{(n |T(x)|)^k}
    \right]
    &\leq
    \left(
      \frac{\lambda^d}{n}
    \right)^{\I \left\{ k = 1 \right\}}
    \left(
      \frac{3 \lambda^{2d} \log n}{n^2}
    \right)^{\I \left\{ k \geq 2 \right\}}
    \prod_{j=1}^{d} \frac{1}{x_j (1-x_j)}.
  \end{align*}
  %
\end{lemma}

\begin{proof}[Lemma~\ref{lem:moment_cell}]

  By \citet[Proposition~1]{mourtada2020minimax},
  $|T(x)| = \prod_{j=1}^{d}
  \left(
    \left(\frac{1}{\lambda} E_{j1} \right) \wedge x_j
    + \left( \frac{1}{\lambda} E_{j2} \right) \wedge (1-x_j)
  \right)$
  where $E_{j1}$ and $E_{j2}$
  are mutually independent $\Exp(1)$ random variables.
  Thus for $0<t<1$,
  using the fact that $E_{j1} + E_{j2} \sim \Gam(2, 1)$,
  %
  \begin{align*}
    \E \left[
      \frac{1}{1 \vee (n |T(x)|)^k}
    \right]
    &\leq
    \frac{1}{n^k}
    \E \left[
      \frac{\I\{\min_j (E_{j1} + E_{j2}) \geq t\}}{|T(x)|^k}
    \right]
    + \P \left(\min_{1 \leq j \leq d} (E_{j1} + E_{j2}) < t\right) \\
    &\leq
    \frac{1}{n^k}
    \prod_{j=1}^d
    \E \left[
      \frac{\I\{E_{j1} + E_{j2} \geq t\}}
      {\left(\frac{1}{\lambda} E_{j1} \wedge x_j
      + \frac{1}{\lambda} E_{j2} \wedge (1-x_j)\right)^k}
    \right]
    + d\, \P \left(E_{j1} < t\right) \\
    &\leq
    \frac{\lambda^{d k}}{n^k}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)}
    \E \left[
      \frac{\I\{E_{j1} + E_{j2} \geq t\}}
      {(E_{j1} +  E_{j2})^k \wedge 1}
    \right]
    + d (1 - e^{-t}) \\
    &\leq
    \frac{\lambda^{d k}}{n^k}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)}
    \int_{t}^{1}
    \frac{e^{-s}}{s^{k-1}}
    \diff s
    + d t \\
    &\leq
    d t
    + \frac{\lambda^{d k}}{n^k}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)}
    \times
    \begin{cases}
      1-t     & \text{if } k = 1  \\
      -\log t & \text{if } k = 2.
    \end{cases}
  \end{align*}
  %
  If $k>2$ we simply use
  $\frac{1}{1 \vee (n |T(x)|)^k} \leq \frac{1}{1 \vee (n |T(x)|)^{k-1}}$
  to reduce the value of $k$.
  Now if $k = 1$ we let $t \to 0$, giving
  %
  \begin{align*}
    \E \left[
      \frac{1}{1 \vee (n |T(x)|)}
    \right]
    &\leq
    \frac{\lambda^d}{n}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)},
  \end{align*}
  %
  and if $k = 2$ then we set $t = 1/n^2$ so that for
  sufficiently large $n$,
  %
  \begin{align*}
    \E \left[
      \frac{1}{1 \vee (n |T(x)|)^2}
    \right]
    &\leq
    \frac{d}{n^2}
    + \frac{2 \lambda^{2d} \log n}{n^2}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)}
    \leq
    \frac{3 \lambda^{2d} \log n}{n^2}
    \prod_{j=1}^d
    \frac{1}{x_j(1-x_j)}.
  \end{align*}
  %
  Lower bounds which match up to
  constants for the first moment
  and up to logarithmic terms for the second moment
  are easily obtained by noting
  $\E \left[ 1 \wedge \frac{1}{(n|T(x)|)^2} \right]
  \geq \E \left[ 1 \wedge \frac{1}{n|T(x)|} \right]^2$
  by Jensen's inequality and
  %
  \begin{align*}
    \E \left[ 1 \wedge \frac{1}{n|T(x)|} \right]
    &\geq \frac{1}{1 + n \E \left[ |T(x)| \right]}
    \geq \frac{1}{1 + 2^d n / \lambda^d}
    \gtrsim \frac{\lambda^d}{n}.
  \end{align*}
\end{proof}

The ongoing endeavor to bound
moments of (products of) $\frac{\I_b(x)}{N_b(x)}$
is concluded with the next result,
chaining together the previous two lemmas to provide an
explicit bound with no expectations on the right-hand side.

\begin{lemma}[Simplified generalized moment bound for
  Mondrian random forest denominators]%
  \label{lem:simple_moment_denominator}
  %
  Grant Assumptions~\ref{ass:mondrian_data}
  and \ref{ass:estimator}.
  Let $T_b \sim \cM\big([0,1]^d, \lambda\big)$
  and $k_b \geq 1$ for $1 \leq b \leq B_0$.
  Then with $k = \sum_{b=1}^{B_0} k_b$,
  %
  \begin{align*}
    &\E\left[
      \prod_{b=1}^{B_0}
      \frac{\I_b(x)}{N_b(x)^{k_b}}
    \right] \\
    &\quad\leq
    \left( \frac{36k}{\inf_{x \in [0,1]^d} f(x)} \right)^{2^{B_0} k}
    \left(
      \prod_{j=1}^{d} \frac{1}{x_j (1-x_j)}
    \right)^{B_0}
    \prod_{b=1}^{B_0}
    \left(
      \frac{\lambda^d}{n}
    \right)^{\I \left\{ k_b = 1 \right\}}
    \left(
      \frac{\lambda^{2d} \log n}{n^2}
    \right)^{\I \left\{ k_b \geq 2 \right\}}
  \end{align*}
  %
  for sufficiently large $n$.
  %
\end{lemma}

\begin{proof}[Lemma~\ref{lem:simple_moment_denominator}]
  This follows directly from
  Lemmas~\ref{lem:moment_denominator} and \ref{lem:moment_cell}.
\end{proof}

Our final preliminary lemma is concerned with further properties of
the inverse truncated binomial distribution, again with the aim
of analyzing $\frac{\I_b(x)}{N_b(x)}$.
This time, instead of merely upper bounding the moments,
we aim to give convergence results for those moments,
again in terms of moments of $\frac{1}{n |T_b(x)|}$.
This time we only need to handle the first
and second moment, so this result does not strictly generalize
Lemma~\ref{lem:binomial_bound} except in simple cases.
The proof is by Taylor's theorem and the Cauchy--Schwarz inequality,
using explicit expressions for moments of the binomial distribution
and bounds from Lemma~\ref{lem:binomial_bound}.

\begin{lemma}[Expectation inequalities for the binomial distribution]%
  \label{lem:binomial_expectation}
  Let $N \sim \Bin(n, p)$ and take $a, b \geq 1$. Then
  %
  \begin{align*}
    0
    &\leq
    \E \left[
      \frac{1}{N+a}
    \right]
    - \frac{1}{n p+a}
    \leq
    \frac{2^{19}}{(n p+a)^2}, \\
    0
    &\leq
    \E \left[
      \frac{1}{(N+a)(N+b)}
    \right]
    - \frac{1}{(n p+a)(n p+b)}
    \leq
    \frac{2^{27}}{(n p +a)(n p +b)}
    \left(
      \frac{1}{n p + a}
      + \frac{1}{n p + b}
    \right).
  \end{align*}

\end{lemma}

\begin{proof}[Lemma~\ref{lem:binomial_expectation}]

  For the first result,
  Taylor's theorem with Lagrange remainder
  applied to $N \mapsto \frac{1}{N+a}$ around $n p$ gives
  %
  \begin{align*}
    \E \left[
      \frac{1}{N+a}
    \right]
    &=
    \E \left[
      \frac{1}{n p+a}
      - \frac{N - n p}{(n p+a)^2}
      + \frac{(N - n p)^2}{(\xi+a)^3}
    \right]
  \end{align*}
  %
  for some $\xi$ between $n p$ and $N$.
  The second term on the right-hand side is zero-mean,
  clearly showing the non-negativity part of the result,
  and applying the Cauchy--Schwarz inequality
  to the remaining term gives
  %
  \begin{align*}
    \E \left[
      \frac{1}{N+a}
    \right]
    - \frac{1}{n p+a}
    &\leq
    \E \left[
      \frac{(N - n p)^2}{(n p+a)^3}
      + \frac{(N - n p)^2}{(N+a)^3}
    \right] \\
    &\leq
    \frac{\E\big[(N - n p)^2\big]}{(n p+a)^3}
    + \sqrt{
      \E\big[(N - n p)^4\big]
      \E \left[
        \frac{1}{(N+a)^6}
    \right]}.
  \end{align*}
  %
  Now we use
  $\E\big[(N - n p)^4\big] \leq n p(1+3n p)$
  and apply Lemma~\ref{lem:binomial_bound} to see that
  %
  \begin{align*}
    \E \left[
      \frac{1}{N+a}
    \right]
    - \frac{1}{n p+a}
    &\leq
    \frac{n p}{(n p+a)^3}
    + \sqrt{\frac{54^6 n p(1+3 n p)}{(n p + a)^6}}
    \leq
    \frac{2^{19}}{(n p+a)^2}.
  \end{align*}
  %
  For the second result,
  Taylor's theorem applied to $N \mapsto \frac{1}{(N+a)(N+b)}$
  around $n p$ gives
  %
  \begin{align*}
    \E \left[
      \frac{1}{(N+a)(N+b)}
    \right]
    &=
    \E \left[
      \frac{1}{(n p+a)(n p + b)}
      - \frac{(N - n p)(2 n p + a + b)}{(n p + a)^2 (n p + b)^2}
    \right] \\
    &\quad+
    \E \left[
      \frac{(N - n p)^2}{(\xi+a)(\xi+b)}
      \left(
        \frac{1}{(\xi + a)^2}
        + \frac{1}{(\xi + a)(\xi + b)}
        + \frac{1}{(\xi + b)^2}
      \right)
    \right]
  \end{align*}
  %
  for some $\xi$ between $n p$ and $N$.
  The second term on the right-hand side is zero-mean,
  clearly showing the non-negativity part of the result,
  and applying the Cauchy--Schwarz inequality
  to the remaining term gives
  %
  \begin{align*}
    &\E \left[
      \frac{1}{(N+a)(N+b)}
    \right]
    - \frac{1}{n p+a} \\
    &\quad\leq
    \E \left[
      \frac{2 (N - n p)^2}{(N+a)(N+b)}
      \left(
        \frac{1}{(N + a)^2}
        + \frac{1}{(N + b)^2}
      \right)
    \right] \\
    &\qquad+
    \E \left[
      \frac{2 (N - n p)^2}{(n p +a)(n p +b)}
      \left(
        \frac{1}{(n p + a)^2}
        + \frac{1}{(n p + b)^2}
      \right)
    \right] \\
    &\quad\leq
    \sqrt{
      4 \E \left[ (N - n p)^4 \right]
      \E \left[
        \frac{1}{(N + a)^6 (N+b)^2}
        + \frac{1}{(N + b)^6 (N+a)^2}
    \right]} \\
    &\qquad+
    \frac{2 \E\big[(N - n p)^2\big]}{(n p +a)(n p +b)}
    \left(
      \frac{1}{(n p + a)^2}
      + \frac{1}{(n p + b)^2}
    \right).
  \end{align*}
  %
  Now we use
  $\E\big[(N - n p)^4\big] \leq n p(1+3n p)$
  and apply Lemma~\ref{lem:binomial_bound} to see that
  %
  \begin{align*}
    \E \left[
      \frac{1}{(N+a)(N+b)}
    \right]
    - \frac{1}{n p+a}
    &\leq
    \sqrt{
      \frac{4n p (1 + 3n p) \cdot 72^8}{(n p + a)^2 (n p + b)^2}
      \left(
        \frac{1}{(n p + a)^4}
        + \frac{1}{(n p + b)^4}
    \right)} \\
    &\quad+
    \frac{2 n p}{(n p +a)(n p +b)}
    \left(
      \frac{1}{(n p + a)^2}
      + \frac{1}{(n p + b)^2}
    \right) \\
    &\leq
    \frac{2^{27}}{(n p + a) (n p + b)}
    \left(
      \frac{1}{n p + a}
      + \frac{1}{n p + b}
    \right).
  \end{align*}
  %
\end{proof}

\subsection{Proofs for Mondrian random forests}

We give rigorous proofs of the central limit theorem,
bias characterization, and variance estimation
results for the Mondrian random forest estimator without debiasing.

\subsection*{Proof of central limit theorem}

\begin{proof}[Theorem~\ref{thm:clt}]
  Follows from the debiased version.
  See the proof of Theorem~\ref{thm:clt_debiased} and set $J=0$,
  $a_0 = 1$, and $\omega_0 = 1$.
\end{proof}

\subsection*{Proof of bias characterization}

See Section~\ref{sec:overview_proofs} in the main paper
for details on our approach to this proof.

\begin{proof}[Theorem~\ref{thm:bias}]

  \proofparagraph{Removing the dependence on the trees}

  By measurability and with $\mu(X_i) = \E[Y_i \mid X_i]$ almost surely,
  %
  \begin{align*}
    \E \left[ \hat \mu(x) \mid \bX, \bT \right]
    - \mu(x)
    &=
    \frac{1}{B}
    \sum_{b=1}^B
    \sum_{i=1}^n \big( \mu(X_i) - \mu(x) \big)
    \frac{\I_{i b}(x)}{N_b(x)}.
  \end{align*}
  %
  Now conditional on $\bX$,
  the terms in the outer sum depend only on $T_b$ so are i.i.d.
  Since $\mu$ is Lipschitz,
  %
  \begin{align*}
    \Var \left[
      \E \left[ \hat \mu(x) \mid \bX, \bT \right]
      - \mu(x)
      \mid \bX
    \right]
    &\leq
    \frac{1}{B}
    \E \left[
      \left(
        \sum_{i=1}^n \big( \mu(X_i) - \mu(x) \big)
        \frac{\I_{i b}(x)}{N_b(x)}
      \right)^2
      \Bigm| \bX
    \right] \\
    &\lesssim
    \frac{1}{B}
    \E \left[
      \max_{1 \leq i \leq n}
      \big\| X_i - x \big\|_2^2
      \left(
        \sum_{i=1}^n
        \frac{\I_{i b}(x)}{N_b(x)}
      \right)^2
      \Bigm| \bX
    \right] \\
    &\lesssim
    \frac{1}{B}
    \sum_{j=1}^{d}
    \E \left[
      |T(x)_j|^2
    \right]
    \lesssim
    \frac{1}{\lambda^2 B},
  \end{align*}
  %
  where we used the law of $T(x)_j$ from
  \citet[Proposition~1]{mourtada2020minimax}.
  So by Chebyshev's inequality,
  %
  \begin{align*}
    \big|
    \E \left[ \hat \mu(x) \mid \bX, \bT \right]
    - \E \left[ \hat \mu(x) \mid \bX \right]
    \big|
    &\lesssim_\P
    \frac{1}{\lambda \sqrt B}.
  \end{align*}

  \proofparagraph{Showing the conditional bias converges in probability}

  Now $\E \left[ \hat\mu(x) \mid \bX \right]$
  is a non-linear function of the i.i.d.\ random variables $X_i$,
  so we use the Efron--Stein inequality
  \citep{efron1981jackknife} to bound its variance.
  Let $\tilde X_{i j} = X_i$ if $i \neq j$ and be an
  independent copy of $X_j$, denoted $\tilde X_j$, if $i = j$.
  Write $\tilde \bX_j = (\tilde X_{1j}, \ldots, \tilde X_{n j})$
  and similarly
  $\tilde \I_{i j b}(x) = \I \big\{ \tilde X_{i j} \in T_b(x) \big\}$
  and $N_{j b}(x) = \sum_{i=1}^{n} \tilde \I_{i j b}(x)$.
  %
  \begin{align}
    \nonumber
    &\Var \left[
      \sum_{i=1}^{n}
      \big( \mu(X_i) - \mu(x) \big)
      \E \left[
        \frac{\I_{i b}(x)}{N_b(x)}
        \Bigm| \bX
      \right]
    \right] \\
    \nonumber
    &\quad\leq
    \frac{1}{2}
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \sum_{i=1}^{n}
        \big( \mu(X_i) - \mu(x) \big)
        \E \left[
          \frac{\I_{i b}(x)}{N_b(x)}
          \Bigm| \bX
        \right]
        \right.\right. \\
        \nonumber
        &\qquad\qquad\qquad\quad\left.\left.
        - \sum_{i=1}^{n}
        \left( \mu(\tilde X_{i j}) - \mu(x) \right)
        \E \left[
          \frac{\tilde \I_{i j b}(x)}{\tilde N_{j b}(x)}
          \Bigm| \tilde \bX_j
        \right]
      \right)^2
    \right] \\
    \nonumber
    &\quad\leq
    \frac{1}{2}
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \sum_{i=1}^{n}
        \left(
          \big( \mu(X_i) - \mu(x) \big)
          \frac{\I_{i b}(x)}{N_b(x)}
          - \left( \mu(\tilde X_{i j}) - \mu(x) \right)
          \frac{\tilde \I_{i j b}(x)}{\tilde N_{j b}(x)}
        \right)
      \right)^2
    \right] \\
    \nonumber
    &\quad\leq
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \sum_{i \neq j}
        \big( \mu(X_i) - \mu(x) \big)
        \left(
          \frac{\I_{i b}(x)}{N_b(x)} - \frac{\I_{i b}(x)}{\tilde N_{j b}(x)}
        \right)
      \right)^2
    \right] \\
    \label{eq:bias_efron_stein}
    &\qquad+
    2 \sum_{j=1}^{n}
    \E \left[
      \left( \mu(X_j) - \mu(x) \right)^2
      \frac{\I_{j b}(x)}{N_b(x)^2}
    \right].
  \end{align}
  %
  For the first term in \eqref{eq:bias_efron_stein} to be non-zero,
  we must have $|N_b(x) - \tilde N_{j b}(x)| = 1$.
  Writing $N_{-j b}(x) = \sum_{i \neq j} \I_{i b}(x)$,
  we may assume by symmetry that
  $\tilde N_{j b}(x) = N_{-j b}(x)$ and $N_b(x) = N_{-j b}(x) + 1$,
  and also that $\I_{j b}(x) = 1$.
  Hence since $f$ is bounded and $\mu$ is Lipschitz,
  writing $\I_{-j b}(x) = \I \left\{ N_{-j b}(x) \geq 1 \right\}$,
  %
  \begin{align*}
    &\sum_{j=1}^{n}
    \E \left[
      \left(
        \sum_{i \neq j}
        \left( \mu(X_i) - \mu(x) \right)
        \left(
          \frac{\I_{i b}(x)}{N_b(x)} - \frac{\I_{i b}(x)}{\tilde N_{j b}(x)}
        \right)
      \right)^2
    \right] \\
    &\quad\lesssim
    \sum_{j=1}^{n}
    \E \left[
      \max_{1 \leq l \leq d}
      |T_b(x)_l|^2
      \left(
        \frac{\sum_{i \neq j}\I_{i b}(x) \I_{j b}(x)}
        {N_{-j b}(x)(N_{-j b}(x) + 1)}
      \right)^2
    \right]
    \lesssim
    \E \left[
      \max_{1 \leq l \leq d}
      |T_b(x)_l|^2
      \frac{\I_{b}(x)}{N_{b}(x)}
    \right].
  \end{align*}
  %
  For $t > 0$, partition by the event
  $\left\{ \max_{1 \leq l \leq d} |T_b(x)_l| \geq t/\lambda \right\}$
  and apply Lemma~\ref{lem:largest_cell} and
  Lemma~\ref{lem:simple_moment_denominator}:
  %
  \begin{align*}
    \E \left[
      \max_{1 \leq l \leq d}
      |T_b(x)_l|^2
      \frac{\I_{b}(x)}{N_{b}(x)}
    \right]
    &\leq
    \P \left(
      \max_{1 \leq l \leq d} |T_b(x)_l| \geq t/\lambda
    \right)
    + (t / \lambda)^2\,
    \E \left[
      \frac{\I_{b}(x)}{N_{b}(x)}
    \right] \\
    &\lesssim
    e^{-t/2}
    + \left( \frac{t}{\lambda} \right)^2
    \frac{\lambda^d}{n}
    \lesssim
    \frac{1}{n^2}
    + \frac{(\log n)^2}{\lambda^2}
    \frac{\lambda^d}{n}
    \lesssim
    \frac{(\log n)^2}{\lambda^2}
    \frac{\lambda^{d}}{n},
  \end{align*}
  %
  where we set $t = 4 \log n$.
  For the second term in \eqref{eq:bias_efron_stein} we have
  %
  \begin{align*}
    \sum_{j=1}^{n}
    \E \left[
      \left( \mu(X_j) - \mu(x) \right)^2
      \frac{\I_{j b}(x)}{N_b(x)^2}
    \right]
    &\lesssim
    \E \left[
      \max_{1 \leq l \leq d}
      |T_b(x)_l|^{2}
      \frac{\I_{b}(x)}{N_b(x)}
    \right]
    \lesssim
    \frac{(\log n)^2}{\lambda^2}
    \frac{\lambda^{d}}{n}
  \end{align*}
  %
  in the same manner.
  Hence
  %
  \begin{align*}
    \Var \left[
      \sum_{i=1}^{n}
      \left( \mu(X_i) - \mu(x) \right)
      \E \left[
        \frac{\I_{i b}(x)}{N_b(x)}
        \Bigm| \bX
      \right]
    \right]
    &\lesssim
    \frac{(\log n)^2}{\lambda^2}
    \frac{\lambda^{d}}{n},
  \end{align*}
  %
  and so by Chebyshev's inequality,
  %
  \begin{align*}
    \big|
    \E \left[ \hat \mu(x) \mid \bX, \bT \right]
    - \E \left[ \hat \mu(x) \right]
    \big|
    &\lesssim_\P
    \frac{1}{\lambda \sqrt B}
    + \frac{\log n}{\lambda}
    \sqrt{ \frac{\lambda^{d}}{n} }.
  \end{align*}

  \proofparagraph{Computing the limiting bias}

  It remains to compute the limit of
  $\E \left[ \hat \mu(x) \right] - \mu(x)$.
  Let $\bX_{-i} = (X_1, \ldots, X_{i-1}, X_{i+1}, \ldots, X_n)$
  and $N_{-i b}(x) = \sum_{j=1}^n \I\{j \neq i\} \I\{X_j \in T_b(x)\}$.
  Then
  %
  \begin{align*}
    \E \left[ \hat \mu(x) \right]
    - \mu(x)
    &=
    \E \left[
      \sum_{i=1}^{n}
      \left( \mu(X_i) - \mu(x) \right)
      \frac{\I_{i b}(x)}{N_b(x)}
    \right] \\
    &=
    \sum_{i=1}^{n}
    \E \left[
      \E \left[
        \frac{\left( \mu(X_i) - \mu(x) \right)\I_{i b}(x)}
        {N_{-i b}(x) + 1}
        \bigm| \bT, \bX_{-i}
      \right]
    \right] \\
    &=
    n \,
    \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {N_{-i b}(x) + 1}
    \right].
  \end{align*}
  %
  By Lemma~\ref{lem:binomial_expectation}, since
  $N_{-i b}(x) \sim \Bin\left(n-1,
  \int_{T_b(x)} f(s) \diff s \right)$
  given $\bT$
  and $f$ is bounded away from zero,
  %
  \begin{align*}
    \left|
    \E \left[
      \frac{1}{N_{-i b}(x) + 1}
      \Bigm| \bT
    \right]
    - \frac{1}{(n-1) \int_{T_b(x)} f(s) \diff s + 1}
    \right|
    &\lesssim
    \frac{1}{n^2 \left( \int_{T_b(x)} f(s) \diff s \right)^2}
    \wedge 1 \\
    &\lesssim
    \frac{1}{n^2 |T_b(x)|^2}
    \wedge 1
  \end{align*}
  %
  and also
  %
  \begin{align*}
    \left|
    \frac{1}{(n-1) \int_{T_b(x)} f(s) \diff s + 1}
    - \frac{1}{n \int_{T_b(x)} f(s) \diff s}
    \right|
    &\lesssim
    \frac{1}{n^2 \left( \int_{T_b(x)} f(s) \diff s\right)^2}
    \wedge 1
    \lesssim
    \frac{1}{n^2 |T_b(x)|^2}
    \wedge 1.
  \end{align*}
  %
  So by Lemma~\ref{lem:largest_cell}
  and Lemma~\ref{lem:moment_cell},
  since $f$ is Lipschitz and bounded,
  using the Cauchy--Schwarz inequality,
  %
  \begin{align*}
    &\left|
    \E \left[ \hat \mu(x) \right]
    - \mu(x)
    - \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {\int_{T_b(x)} f(s) \diff s}
    \right]
    \right|
    \lesssim
    \E \left[
      \frac{n \int_{T_b(x)} \left| \mu(s) - \mu(x) \right| f(s) \diff s}
      {n^2 |T_b(x)|^2 \vee 1}
    \right] \\
    &\qquad\lesssim
    \E \left[
      \frac{\max_{1 \leq l \leq d} |T_b(x)_l| }
      {n |T_b(x)| \vee 1}
    \right] \\
    &\qquad\lesssim
    \frac{2 \log n}{\lambda} \,
    \E \left[
      \frac{1}{n |T_b(x)| \vee 1}
    \right]
    + \P \left( \max_{1 \leq l \leq d} |T_b(x)_l| >
    \frac{2 \log n}{\lambda} \right)^{1/2}
    \E \left[
      \frac{1}
      {n^2 |T_b(x)|^2 \vee 1}
    \right]^{1/2} \\
    &\qquad\lesssim
    \frac{\log n}{\lambda} \,
    \frac{\lambda^d}{n}
    + \frac{d}{n}
    \frac{\lambda^d \sqrt{\log n}}{n}
    \lesssim
    \frac{\log n}{\lambda} \,
    \frac{\lambda^d}{n}.
  \end{align*}
  %
  Next set
  $A = \frac{1}{f(x) |T_b(x)|} \int_{T_b(x)} (f(s) - f(x)) \diff s
  \geq \inf_{s \in [0,1]^d} \frac{f(s)}{f(x)} - 1$.
  Use the Maclaurin series of $\frac{1}{1+x}$
  up to order $\flbeta$ to see
  $\frac{1}{1 + A} = \sum_{k=0}^{\flbeta} (-1)^k A^k
  + O \left( A^{\flbeta + 1} \right)$.
  Hence
  %
  \begin{align*}
    &\E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {\int_{T_b(x)} f(s) \diff s}
    \right] \\
    &\quad=
    \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {f(x) |{T_b(x)}|}
      \frac{1}{1 + A}
    \right] \\
    &\quad=
    \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {f(x) |{T_b(x)}|}
      \left(
        \sum_{k=0}^{\flbeta}
        (-1)^k
        A^k
        + O \left( |A|^{\flbeta + 1} \right)
      \right)
    \right].
  \end{align*}
  %
  Note that since $f$ and $\mu$ are Lipschitz
  and by integrating the tail probability given in Lemma~\ref{lem:largest_cell},
  the Maclaurin remainder term is bounded by
  %
  \begin{align*}
    &\E \left[
      \frac{\int_{T_b(x)} \left| \mu(s) - \mu(x) \right| f(s) \diff s}
      {f(x) |{T_b(x)}|}
      |A|^{\flbeta + 1}
    \right] \\
    &\qquad=
    \E \left[
      \frac{\int_{T_b(x)} \left| \mu(s) - \mu(x) \right| f(s) \diff s}
      {f(x) |{T_b(x)}|}
      \left(
        \frac{1}{f(x) |{T_b(x)}|} \int_{T_b(x)} (f(s) - f(x)) \diff s
      \right)^{\flbeta + 1}
    \right] \\
    &\qquad\lesssim
    \E \left[
      \max_{1 \leq l \leq d}
      |T_b(x)_l|^{\flbeta+2}
    \right]
    =
    \int_{0}^{\infty}
    \P \left(
      \max_{1 \leq l \leq d}
      |T_b(x)_l|
      \geq t^{\frac{1}{\flbeta+2}}
    \right)
    \diff t
    \leq
    \int_{0}^{\infty}
    2 d e^{- \lambda t^{\frac{1}{\flbeta+2}} / 2}
    \diff t \\
    &\qquad=
    \frac{2^{\flbeta + 3} d (\flbeta + 2)! }
    {\lambda^{\flbeta + 2}}
    \lesssim
    \frac{1}{\lambda^{\beta}}
  \end{align*}
  %
  since
  $\int_0^\infty e^{-a x^{1/k}} \diff x
  = a^{-k} k!$.
  Hence to summarize the progress so far, we have
  %
  \begin{align*}
    &\left|
    \E \left[
      \hat \mu(x)
    \right]
    - \mu(x)
    - \sum_{k=0}^{\flbeta}
    (-1)^k \,
    \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {f(x)^{k+1} |T_b(x)|^{k+1}}
      \left(
        \int_{T_b(x)} (f(s) - f(x)) \diff s
      \right)^k
    \right]
    \right| \\
    &\qquad\lesssim
    \frac{\log n}{\lambda}
    \frac{\lambda^d}{n}
    + \frac{1}{\lambda^\beta}.
  \end{align*}
  %
  We continue to evaluate this expectation.
  First, by Taylor's theorem and with
  $\nu$ a multi-index,
  since $f \in \cH^\beta$,
  %
  \begin{align*}
    \left(
      \int_{T_b(x)} (f(s) - f(x)) \diff s
    \right)^k
    &=
    \left(
      \sum_{|\nu| = 1}^\flbeta
      \frac{\partial^\nu f(x)}{\nu !}
      \int_{T_b(x)}
      (s - x)^\nu
      \diff s
    \right)^k \\
    &\quad+
    O \left(
      |T_b(x)| \max_{1 \leq l \leq d} |T_b(x)_l|^\beta
    \right).
  \end{align*}
  %
  Next, by the multinomial theorem
  with a multi-index $u$ indexed by $\nu$ with $|\nu| \geq 1$,
  %
  \begin{align*}
    \left(
      \sum_{|\nu| = 1}^\flbeta
      \frac{\partial^\nu f(x)}{\nu !}
      \int_{T_b(x)}
      (s - x)^\nu
      \diff s
    \right)^k
    &=
    \sum_{|u| = k}
    \binom{k}{u}
    \left(
      \frac{\partial^\nu f(x)}{\nu !}
      \int_{T_b(x)} (s-x)^\nu \diff s
    \right)^u
  \end{align*}
  %
  where $\binom{k}{u}$ is a multinomial coefficient.
  By Taylor's theorem with $f, \mu \in \cH^\beta$,
  %
  \begin{align*}
    &\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s \\
    &\quad=
    \sum_{|\nu'|=1}^{\flbeta}
    \sum_{|\nu''|=0}^{\flbeta}
    \frac{\partial^{\nu'} \mu(x)}{\nu' !}
    \frac{\partial^{\nu''} f(x)}{\nu'' !}
    \int_{T_b(x)} (s-x)^{\nu' + \nu''} \diff s
    + O \left( |T_b(x)| \max_{1 \leq l \leq d} |T_b(x)_l|^\beta \right).
  \end{align*}
  %
  Now by integrating the tail probabilities in Lemma~\ref{lem:largest_cell},
  $ \E \left[ \max_{1 \leq l \leq d} |T_b(x)_l|^\beta \right]
  \lesssim \frac{1}{\lambda^\beta}$.
  Therefore by Lemma~\ref{lem:moment_cell},
  writing $T_b(x)^\nu$ for $\int_{T_b(x)} (s-x)^\nu \diff s$,
  %
  \begin{align*}
    &\sum_{k=0}^{\flbeta}
    (-1)^k \,
    \E \left[
      \frac{\int_{T_b(x)} \left( \mu(s) - \mu(x) \right) f(s) \diff s}
      {f(x)^{k+1} |T_b(x)|^{k+1}}
      \left(
        \int_{T_b(x)} (f(s) - f(x)) \diff s
      \right)^k
    \right] \\
    &\,=
    \sum_{k=0}^{\flbeta}
    (-1)^k \,
    \E
    \left[
      \frac{
        \sum_{|\nu'|=1}^{\flbeta}
        \sum_{|\nu''|=0}^{\flbeta}
        \frac{\partial^{\nu'} \mu(x)}{\nu' !}
        \frac{\partial^{\nu''} f(x)}{\nu'' !}
        T_b(x)^{\nu' + \nu''}
      }{f(x)^{k+1} |T_b(x)|^{k+1}}
      \sum_{|u| = k}
      \binom{k}{u}
      \left(
        \frac{\partial^\nu f(x)}{\nu !}
        T_b(x)^\nu
      \right)^u
    \right] \\
    &\qquad+
    O \left(
      \frac{1}{\lambda^\beta}
    \right) \\
    &\,=
    \sum_{|\nu'|=1}^{\flbeta}
    \sum_{|\nu''|=0}^{\flbeta}
    \sum_{|u|=0}^{\flbeta}
    \frac{\partial^{\nu'} \mu(x)}{\nu' !}
    \frac{\partial^{\nu''} f(x)}{\nu'' !}
    \left( \frac{\partial^\nu f(x)}{\nu !} \right)^u
    \binom{|u|}{u} \\
    &\qquad\times
    \frac{(-1)^{|u|}}{f(x)^{|u|+1}}
    \E \left[
      \frac{ T_b(x)^{\nu' + \nu''} (T_b(x)^\nu)^u}{|T_b(x)|^{|u|+1}}
    \right]
    + O \left(
      \frac{1}{\lambda^\beta}
    \right) .
  \end{align*}
  %
  Now we show this is a polynomial in $\lambda$.
  For $1 \leq j \leq d$, define the independent variables
  $E_{1j*} \sim \Exp(1) \wedge (\lambda x_j)$
  and $E_{2j*} \sim \Exp(1) \wedge (\lambda (1-x_j))$
  so
  $T_b(x) = \prod_{j=1}^{d} [x_j - E_{1j*} / \lambda, x_j + E_{2j*} / \lambda]$.
  Then
  %
  \begin{align*}
    T_b(x)^\nu
    &=
    \int_{T_b(x)} (s-x)^\nu \diff s
    = \prod_{j=1}^d
    \int_{x_j - E_{1j*}/\lambda}^{x_j+E_{2j*}/\lambda}
    (s - x_j)^{\nu_j} \diff s
    = \prod_{j=1}^d
    \int_{-E_{1j*}}^{E_{2j*}} (s / \lambda)^{\nu_j} 1/\lambda \diff s \\
    &=
    \lambda^{-d - |\nu|}
    \prod_{j=1}^d
    \int_{-E_{1j*}}^{E_{2j*}} s^{\nu_j} \diff s
    = \lambda^{-d - |\nu|}
    \prod_{j=1}^d
    \frac{E_{2j*}^{\nu_j + 1} + (-1)^{\nu_j} E_{1j*}^{\nu_j + 1}}
    {\nu_j + 1}.
  \end{align*}
  %
  So by independence over $j$,
  %
  \begin{align}
    \nonumber
    &\E \left[
      \frac{ T_b(x)^{\nu' + \nu''} (T_b(x)^\nu)^u}{|T_b(x)|^{|u|+1}}
    \right] \\
    \label{eq:bias_calc}
    &\quad=
    \lambda^{- |\nu'| - |\nu''| - |\nu| \cdot u}
    \prod_{j=1}^d
    \E \left[
      \frac{E_{2j*}^{\nu'_j + \nu''_j + 1}
      + (-1)^{\nu'_j + \nu''_j} E_{1j*}^{\nu'_j + \nu''_j + 1}}
      {(\nu'_j + \nu''_j + 1) (E_{2j*} + E_{1j*})}
      \frac{\left(E_{2j*}^{\nu_j + 1}
      + (-1)^{\nu_j} E_{1j*}^{\nu_j + 1}\right)^u}
      {(\nu_j + 1)^u (E_{2j*} + E_{1j*})^{|u|}}
    \right].
  \end{align}
  %
  The final step is to replace $E_{1j*}$
  by $E_{1j} \sim \Exp(1)$ and similarly for $E_{2j*}$.
  Note that for a positive constant $C$,
  %
  \begin{align*}
    \P \left(
      \bigcup_{j=1}^{d}
      \left(
        \left\{
          E_{1j*} \neq E_{1j}
        \right\}
        \cup
        \left\{
          E_{2j*} \neq E_{2j}
        \right\}
      \right)
    \right)
    &\leq
    2d\,
    \P \left(
      \Exp(1) \geq \lambda \min_{1 \leq j \leq d}
      (x_j \wedge (1-x_j))
    \right) \\
    &\leq
    2d e^{-C \lambda}.
  \end{align*}
  %
  Further, the quantity inside the expectation in \eqref{eq:bias_calc}
  is bounded almost surely by one and so
  the error incurred by replacing
  $E_{1j*}$ and $E_{2j*}$ by $E_{1j}$ and $E_{2j}$
  in \eqref{eq:bias_calc}
  is at most $2 d e^{-C \lambda} \lesssim \lambda^{-\beta}$.
  Thus the limiting bias is
  %
  \begin{align}
    \nonumber
    &\E \left[ \hat \mu(x) \right]
    - \mu(x) \\
    \nonumber
    &\quad=
    \sum_{|\nu'|=1}^{\flbeta}
    \sum_{|\nu''|=0}^{\flbeta}
    \sum_{|u|=0}^{\flbeta}
    \frac{\partial^{\nu'} \mu(x)}{\nu' !}
    \frac{\partial^{\nu''} f(x)}{\nu'' !}
    \left( \frac{\partial^\nu f(x)}{\nu !} \right)^u
    \binom{|u|}{u}
    \frac{(-1)^{|u|}}{f(x)^{|u|+1}}
    \, \lambda^{- |\nu'| - |\nu''| - |\nu| \cdot u} \\
    \nonumber
    &\qquad\times
    \prod_{j=1}^d
    \E \left[
      \frac{E_{2j}^{\nu'_j + \nu''_j + 1}
      + (-1)^{\nu'_j + \nu''_j} E_{1j}^{\nu'_j + \nu''_j + 1}}
      {(\nu'_j + \nu''_j + 1) (E_{2j} + E_{1j})}
      \frac{\left(E_{2j}^{\nu_j + 1}
      + (-1)^{\nu_j} E_{1j}^{\nu_j + 1}\right)^u}
      {(\nu_j + 1)^u (E_{2j} + E_{1j})^{|u|}}
    \right] \\
    \label{eq:bias}
    &\qquad+
    O \left( \frac{\log n}{\lambda} \frac{\lambda^d}{n} \right)
    + O \left( \frac{1}{\lambda^\beta} \right),
  \end{align}
  %
  recalling that $u$ is a multi-index which is indexed by the multi-index $\nu$.
  This is a polynomial in $\lambda$ of degree at most $\flbeta$,
  since higher-order terms can be absorbed into $O(1 / \lambda^\beta)$,
  which has finite coefficients depending only on
  the derivatives up to order $\flbeta$ of $f$ and $\mu$ at $x$.
  Now we show that the odd-degree terms in this polynomial are all zero.
  Note that a term is of odd degree if and only if
  $|\nu'| + |\nu''| + |\nu| \cdot u$ is odd.
  This implies that there exists $1 \leq j \leq d$ such that
  exactly one of either
  $\nu'_j + \nu''_j$ is odd or
  $\sum_{|\nu|=1}^{\flbeta} \nu_j u_\nu$ is odd.

  If $\nu'_j + \nu''_j$ is odd, then
  $\sum_{|\nu|=1}^{\flbeta} \nu_j u_\nu$ is even, so
  $|\{\nu : \nu_j u_\nu \text{ is odd}\}|$ is even.
  Consider the effect of swapping $E_{1j}$ and $E_{2j}$,
  an operation which by independence preserves their joint law,
  in each of
  %
  \begin{align}
    \label{eq:bias_odd_1}
    \frac{E_{2j}^{\nu'_j + \nu''_j + 1}
    + (-1)^{\nu'_j + \nu''_j} E_{1j}^{\nu'_j + \nu''_j + 1}}
    {E_{2j} + E_{1j}}
  \end{align}
  %
  and
  %
  \begin{align}
    \nonumber
    &\frac{\left(E_{2j}^{\nu_j + 1}
    + (-1)^{\nu_j} E_{1j}^{\nu_j + 1}\right)^u}
    {(E_{2j} + E_{1j})^{|u|}} \\
    \label{eq:bias_odd_2}
    &\quad=
    \prod_{\substack{|\nu| = 1 \\
    \nu_j u_\nu \text{ even}}}^\beta
    \frac{\left(E_{2j}^{\nu_j + 1}
    + (-1)^{\nu_j} E_{1j}^{\nu_j + 1}\right)^{u_\nu}}
    {(E_{2j} + E_{1j})^{u_\nu}}
    \prod_{\substack{|\nu| = 1 \\
    \nu_j u_\nu \text{ odd}}}^\beta
    \frac{\left(E_{2j}^{\nu_j + 1}
    + (-1)^{\nu_j} E_{1j}^{\nu_j + 1}\right)^{u_\nu}}
    {(E_{2j} + E_{1j})^{u_\nu}}.
  \end{align}
  %
  Clearly $\nu'_j + \nu''_j$ being odd inverts the
  sign of \eqref{eq:bias_odd_1}.
  For \eqref{eq:bias_odd_2},
  each term in the first product has either
  $\nu_j$ even or $u_\nu$ even, so its sign is preserved.
  Every term in the second product of \eqref{eq:bias_odd_2}
  has its sign inverted due to both $\nu_j$ and $u_\nu$ being odd,
  but there are an even number of terms,
  preserving the overall sign.
  Therefore the expected product
  of \eqref{eq:bias_odd_1} and \eqref{eq:bias_odd_2} is zero by symmetry.

  If however $\nu'_j + \nu''_j$ is even, then
  $\sum_{|\nu|=1}^{\flbeta} \nu_j u_\nu$ is odd so
  $|\{\nu : \nu_j u_\nu \text{ is odd}\}|$ is odd.
  Clearly the sign of \eqref{eq:bias_odd_1} is preserved.
  Again the sign of the first product in \eqref{eq:bias_odd_2}
  is preserved, and the sign of every term in \eqref{eq:bias_odd_2}
  is inverted. However there are now an odd number of terms in the
  second product, so its overall sign is inverted.
  Therefore the expected product
  of \eqref{eq:bias_odd_1} and \eqref{eq:bias_odd_2} is again zero.

  \proofparagraph{Calculating the second-order bias}

  Next we calculate some special cases, beginning with
  the form of the leading second-order bias,
  where the exponent in $\lambda$ is
  $|\nu'| + |\nu''| + u \cdot |\nu| = 2$,
  proceeding by cases on the values of $|\nu'|$, $|\nu''|$, and $|u|$.
  Firstly, if $|\nu'| = 2$ then $|\nu''| = |u| = 0$.
  Note that if any $\nu'_j = 1$ then the expectation in \eqref{eq:bias} is zero.
  Hence we can assume $\nu'_j \in \{0, 2\}$, yielding
  %
  \begin{align*}
    \frac{1}{2 \lambda^2}
    \sum_{j=1}^d
    \frac{\partial^2 \mu(x)}{\partial x_j^2}
    \frac{1}{3}
    \E \left[
      \frac{E_{2j}^{3} + E_{1j}^{3}} {E_{2j} + E_{1j}}
    \right]
    &=
    \frac{1}{2 \lambda^2}
    \sum_{j=1}^d
    \frac{\partial^2 \mu(x)}{\partial x_j^2}
    \frac{1}{3}
    \E \left[
      E_{1j}^{2}
      + E_{2j}^{2}
      - E_{1j} E_{2j}
    \right] \\
    &=
    \frac{1}{2 \lambda^2}
    \sum_{j=1}^d
    \frac{\partial^2 \mu(x)}{\partial x_j^2},
  \end{align*}
  %
  where we used that $E_{1j}$ and $E_{2j}$ are independent $\Exp(1)$.
  Next we consider $|\nu'| = 1$ and $|\nu''| = 1$, so $|u| = 0$.
  Note that if $\nu'_j = \nu''_{j'} = 1$ with $j \neq j'$ then the
  expectation in \eqref{eq:bias} is zero.
  So we need only consider $\nu'_j = \nu''_j = 1$, giving
  %
  \begin{align*}
    \frac{1}{\lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}
    \frac{1}{3}
    \E \left[
      \frac{E_{2j}^{3} + E_{1j}^{3}}
      {E_{2j} + E_{1j}}
    \right]
    &=
    \frac{1}{\lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}.
  \end{align*}
  %
  Finally we have the case where $|\nu'| = 1$, $|\nu''| = 0$
  and $|u|=1$. Then $u_\nu = 1$ for some $|\nu| = 1$ and zero otherwise.
  Note that if $\nu'_j = \nu_{j'} = 1$ with $j \neq j'$ then the
  expectation is zero. So we need only consider $\nu'_j = \nu_j = 1$, giving
  %
  \begin{align*}
    &- \frac{1}{\lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}
    \frac{1}{4}
    \E \left[
      \frac{(E_{2j}^2 - E_{1j}^2)^2}
      {(E_{2j} + E_{1j})^2}
    \right] \\
    &\quad=
    - \frac{1}{4 \lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}
    \E \left[
      E_{1j}^2
      + E_{2j}^2
      - 2 E_{1j} E_{2j}
    \right]
    =
    - \frac{1}{2 \lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}.
  \end{align*}
  %
  Hence the second-order bias term is
  %
  \begin{align*}
    \frac{1}{2 \lambda^2}
    \sum_{j=1}^d
    \frac{\partial^2 \mu(x)}{\partial x_j^2}
    + \frac{1}{2 \lambda^2}
    \frac{1}{f(x)}
    \sum_{j=1}^{d}
    \frac{\partial \mu(x)}{\partial x_j}
    \frac{\partial f(x)}{\partial x_j}.
  \end{align*}

  \proofparagraph{Calculating the bias if the data is uniformly distributed}

  If $X_i \sim \Unif\big([0,1]^d\big)$ then $f(x) = 1$ and
  the bias expansion from \eqref{eq:bias} becomes
  %
  \begin{align*}
    \sum_{|\nu'|=1}^{\flbeta}
    \lambda^{- |\nu'|}
    \frac{\partial^{\nu'} \mu(x)}{\nu' !}
    \prod_{j=1}^d
    \E \left[
      \frac{E_{2j}^{\nu'_j + 1}
      + (-1)^{\nu'_j} E_{1j}^{\nu'_j + 1}}
      {(\nu'_j + 1) (E_{2j} + E_{1j})}
    \right].
  \end{align*}
  %
  Note that this is zero if any $\nu_j'$ is odd.
  Therefore we can group these terms based on the exponent of $\lambda$ to see
  %
  \begin{align*}
    \frac{B_r(x)}{\lambda^{2r}}
    &=
    \frac{1}{\lambda^{2r}}
    \sum_{|\nu|=r}
    \frac{\partial^{2 \nu} \mu(x)}{(2 \nu) !}
    \prod_{j=1}^d
    \frac{1}{2\nu_j + 1}
    \E \left[
      \frac{E_{2j}^{2\nu_j + 1} + E_{1j}^{2\nu_j + 1}}
      {E_{2j} + E_{1j}}
    \right].
  \end{align*}
  %
  Since $\int_0^\infty \frac{e^{-t}}{a+t} \diff t = e^a \Gamma(0,a)$
  and $\int_0^\infty s^a \Gamma(0, a) \diff s = \frac{a!}{a+1}$,
  with $\Gamma(0, a) = \int_a^\infty \frac{e^{-t}}{t} \diff t$
  the upper incomplete gamma function,
  the expectation is easily calculated as
  %
  \begin{align*}
    \E \left[
      \frac{E_{2j}^{2\nu_j + 1} + E_{1j}^{2\nu_j + 1}}
      {E_{2j} + E_{1j}}
    \right]
    &=
    2
    \int_{0}^{\infty}
    s^{2\nu_j + 1}
    e^{-s}
    \int_{0}^{\infty}
    \frac{e^{-t}}
    {s + t}
    \diff t
    \diff s \\
    &=
    2 \int_{0}^{\infty}
    s^{2\nu_j + 1}
    \Gamma(0, s)
    \diff s
    =
    \frac{(2 \nu_j + 1)!}{\nu_j + 1},
  \end{align*}
  %
  so
  %
  \begin{align*}
    \frac{B_r(x)}{\lambda^{2r}}
    &=
    \frac{1}{\lambda^{2r}}
    \sum_{|\nu|=r}
    \frac{\partial^{2 \nu} \mu(x)}{(2 \nu) !}
    \prod_{j=1}^d
    \frac{1}{2\nu_j + 1}
    \frac{(2 \nu_j + 1)!}{\nu_j + 1}
    =
    \frac{1}{\lambda^{2r}}
    \sum_{|\nu|=r}
    \partial^{2 \nu} \mu(x)
    \prod_{j=1}^d
    \frac{1}{\nu_j + 1}.
  \end{align*}
  %
\end{proof}

\subsection*{Proof of variance estimator consistency}

\begin{proof}[Theorem~\ref{thm:variance_estimation}]
  Follows from the debiased version.
  See the proof of Theorem~\ref{thm:variance_estimation_debiased}
  and set $J=0$,
  $a_0 = 1$, and $\omega_0 = 1$.
\end{proof}

\subsection*{Proof of confidence interval validity}

\begin{proof}[Theorem~\ref{thm:confidence}]
  %
  By Theorem~\ref{thm:bias}
  and Theorem~\ref{thm:variance_estimation},
  %
  \begin{align*}
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu(x) - \mu(x)}{\hat \Sigma(x)^{1/2}}
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu(x) - \E \left[ \hat \mu(x) \mid \bX, \bT \right]}
    {\hat \Sigma(x)^{1/2}}
    + \sqrt{\frac{n}{\lambda^d}}
    \frac{\E \left[ \hat \mu(x) \mid \bX, \bT \right] - \mu(x)}
    {\hat \Sigma(x)^{1/2}} \\
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu(x) - \E \left[ \hat \mu(x) \mid \bX, \bT \right]}
    {\hat \Sigma(x)^{1/2}}
    + \sqrt{\frac{n}{\lambda^d}} \,
    O_\P \left(
      \frac{1}{\lambda^{\beta \wedge 2}}
      + \frac{1}{\lambda \sqrt B}
      + \frac{\log n}{\lambda} \sqrt{\frac{\lambda^d}{n}}
    \right).
  \end{align*}
  %
  The first term now converges weakly to $\cN(0,1)$ by
  Slutsky's theorem, Theorem~\ref{thm:clt},
  and Theorem~\ref{thm:variance_estimation},
  while the second term is $o_\P(1)$ by assumption.
  Validity of the confidence interval follows immediately.
  %
\end{proof}

\subsection{Proofs for debiased Mondrian random forests}

We give rigorous proofs of the central limit theorem,
bias characterization, variance estimation,
confidence interval validity, and minimax optimality
results for the debiased Mondrian random forest estimator.

\subsection*{Proof of central limit theorem with debiasing}

\begin{proof}[Theorem~\ref{thm:clt_debiased}]

  We use the martingale central limit theorem given by
  \citet[Theorem~3.2]{hall2014martingale}.
  For each $1 \leq i \leq n$ define
  $\cH_{n i}$ to be the filtration
  generated by $\bT$, $\bX$ and
  $(\varepsilon_j : 1 \leq j \leq i)$,
  noting that $\cH_{n i} \subseteq \cH_{(n+1)i}$
  because $B$ increases weakly as $n$ increases.
  Let $\I_{i b r}(x) = \I\{X_i \in T_{b r}(x)\}$
  where $T_{b r}(x)$ is the cell containing $x$ in tree $b$
  used to construct $\hat \mu_r(x)$,
  and similarly let $N_{b r}(x) = \sum_{i=1}^n \I_{i b r}(x)$
  and $\I_{b r}(x) = \I\{N_{b r}(x) \geq 1\}$.
  Define the $\cH_{n i}$-measurable and square integrable
  variables
  %
  \begin{align*}
    S_i(x)
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    \omega_r
    \frac{1}{B} \sum_{b=1}^B
    \frac{\I_{i b r}(x) \varepsilon_i} {N_{b r}(x)},
  \end{align*}
  %
  which satisfy the martingale
  difference property
  $\E [ S_i(x) \mid \cH_{n i} ] = 0$.
  Further,
  %
  \begin{align*}
    \sqrt{\frac{n}{\lambda^d}}
    \big(
      \hat\mu_\rd(x)
      - \E\left[
        \hat\mu_\rd(x) \mid \bX, \bT
      \right]
    \big)
    = \sum_{i=1}^n S_i(x).
  \end{align*}
  %
  By \citet[Theorem~3.2]{hall2014martingale}
  it suffices to check that
  %
  \begin{enumerate}[label=(\roman*)]
    \item $\max_i |S_i(x)| \to 0$ in probability,
    \item $\E\left[\max_i S_i(x)^2\right] \lesssim 1$,
    \item $\sum_i S_i(x)^2 \to \Sigma_\rd(x)$ in probability.
  \end{enumerate}

  \proofparagraph{checking condition (i)}
  %
  Since $J$ is fixed and
  $\E[|\varepsilon_i|^3 \mid X_i]$ is bounded,
  by Jensen's inequality and
  Lemma~\ref{lem:simple_moment_denominator},
  %
  \begin{align*}
    \E\left[\max_{1 \leq i \leq n} |S_i(x)| \right]
    &=
    \E\left[\max_{1 \leq i \leq n}
      \left|
      \sqrt{\frac{n}{\lambda^d}}
      \sum_{r=0}^{J}
      \omega_r
      \frac{1}{B} \sum_{b=1}^B
      \frac{\I_{i b r}(x) \varepsilon_i} {N_{b r}(x)}
      \right|
    \right] \\
    &\leq
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    |\omega_r|
    \frac{1}{B}
    \E\left[\max_{1 \leq i \leq n}
      \left|
      \sum_{b=1}^B
      \frac{\I_{i b r}(x) \varepsilon_i} {N_{b r}(x)}
      \right|
    \right] \\
    &\leq
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    |\omega_r|
    \frac{1}{B}
    \E\left[
      \sum_{i=1}^{n}
      \left(
        \sum_{b=1}^B
        \frac{\I_{i b r}(x) |\varepsilon_i|} {N_{b r}(x)}
      \right)^3
    \right]^{1/3} \\
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    |\omega_r|
    \frac{1}{B}
    \E\left[
      \sum_{i=1}^{n}
      |\varepsilon_i|^3
      \sum_{b=1}^B
      \sum_{b'=1}^B
      \sum_{b''=1}^B
      \frac{\I_{i b r}(x) } {N_{b r}(x)}
      \frac{\I_{i b' r}(x) } {N_{b' r}(x)}
      \frac{\I_{i b'' r}(x) } {N_{b'' r}(x)}
    \right]^{1/3} \\
    &\lesssim
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    |\omega_r|
    \frac{1}{B^{2/3}}
    \E\left[
      \sum_{b=1}^B
      \sum_{b'=1}^B
      \frac{\I_{b r}(x)} {N_{b r}(x)}
      \frac{\I_{b' r}(x)} {N_{b' r}(x)}
    \right]^{1/3} \\
    &\lesssim
    \sqrt{\frac{n}{\lambda^d}}
    \sum_{r=0}^{J}
    |\omega_r|
    \frac{1}{B^{2/3}}
    \left(
      B^2 \frac{a_r^{2d} \lambda^{2d}}{n^2}
      + B \frac{a_r^{2d} \lambda^{2d} \log n}{n^2}
    \right)^{1/3} \\
    &\lesssim
    \left( \frac{\lambda^d}{n} \right)^{1/6}
    + \left( \frac{\lambda^d}{n} \right)^{1/6}
    \left( \frac{\log n}{B} \right)^{1/3}
    \to 0.
  \end{align*}

  \proofparagraph{checking condition (ii)}
  %
  Since $\E[\varepsilon_i^2 \mid X_i]$ is bounded
  and by Lemma~\ref{lem:simple_moment_denominator},
  %
  \begin{align*}
    \E\left[\max_{1 \leq i \leq n} S_i(x)^2 \right]
    &=
    \E\left[
      \max_{1 \leq i \leq n}
      \left(
        \sqrt{\frac{n}{\lambda^d}}
        \sum_{r=0}^{J}
        \omega_r
        \frac{1}{B} \sum_{b=1}^B
        \frac{\I_{i b r}(x) \varepsilon_i} {N_{b r}(x)}
      \right)^2
    \right] \\
    &\leq
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    (J+1)^2
    \max_{0 \leq r \leq J}
    \omega_r^2
    \,\E\left[
      \sum_{i=1}^{n}
      \sum_{b=1}^B
      \sum_{b'=1}^B
      \frac{\I_{i b r}(x) \I_{i b' r}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r}(x)}
    \right] \\
    &\lesssim
    \frac{n}{\lambda^d}
    \max_{0 \leq r \leq J}
    \E\left[
      \frac{\I_{b r}(x)}{N_{b r}(x)}
    \right]
    \lesssim
    \frac{n}{\lambda^d}
    \max_{0 \leq r \leq J}
    \frac{a_r^d \lambda^d}{n}
    \lesssim 1.
  \end{align*}

  \proofparagraph{checking condition (iii)}

  Next, we have
  %
  \begin{align}
    \nonumber
    \sum_{i=1}^n
    S_i(x)^2
    &=
    \sum_{i=1}^n
    \left(
      \sqrt{\frac{n}{\lambda^d}}
      \sum_{r=0}^{J}
      \omega_r
      \frac{1}{B} \sum_{b=1}^B
      \frac{\I_{i b r}(x) \varepsilon_i} {N_{b r}(x)}
    \right)^2 \\
    &=
    \nonumber
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    \sum_{i=1}^n
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'}
    \sum_{b=1}^B
    \sum_{b'=1}^B
    \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
    {N_{b r}(x) N_{b' r'}(x)} \\
    &=
    \label{eq:clt_condition_sum}
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    \sum_{i=1}^n
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'}
    \sum_{b=1}^B
    \left(
      \frac{\I_{i b r}(x) \I_{i b r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b r'}(x)}
      + \sum_{b' \neq b}
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right).
  \end{align}
  %
  By boundedness of $\E[\varepsilon_i^2 \mid X_i]$
  and Lemma~\ref{lem:simple_moment_denominator},
  the first term in \eqref{eq:clt_condition_sum}
  converges to zero in probability as
  %
  \begin{align*}
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    \sum_{i=1}^n
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'}
    \sum_{b=1}^B
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b r'}(x)}
    \right]
    &\lesssim
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    \max_{0 \leq r \leq J}
    \sum_{b=1}^B
    \E \left[
      \frac{\I_{b r}(x)}{N_{b r}(x)}
    \right]
    \lesssim
    \frac{1}{B}
    \to 0.
  \end{align*}
  %
  For the second term in \eqref{eq:clt_condition_sum},
  the law of total variance gives
  %
  \begin{align}
    \nonumber
    &\Var \left[
      \frac{n}{\lambda^d}
      \frac{1}{B^2}
      \sum_{i=1}^n
      \sum_{r=0}^{J}
      \sum_{r'=0}^{J}
      \omega_r
      \omega_{r'}
      \sum_{b=1}^B
      \sum_{b' \neq b}
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    \nonumber
    &\quad\leq
    (J+1)^4
    \max_{0 \leq r, r' \leq J}
    \omega_r
    \omega_{r'}
    \Var \left[
      \frac{n}{\lambda^d}
      \frac{1}{B^2}
      \sum_{i=1}^n
      \sum_{b=1}^B
      \sum_{b' \neq b}
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    \nonumber
    &\quad\lesssim
    \max_{0 \leq r, r' \leq J}
    \E \left[
      \Var \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    \label{eq:total_variance}
    &\qquad+
    \max_{0 \leq r, r' \leq J}
    \Var \left[
      \E \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right]
  \end{align}
  %
  For the first term in \eqref{eq:total_variance},
  %
  \begin{align*}
    &\E \left[
      \Var \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    &\quad=
    \frac{n^2}{\lambda^{2d}}
    \frac{1}{B^4}
    \sum_{i=1}^n
    \sum_{j=1}^n
    \sum_{b=1}^B
    \sum_{b' \neq b}
    \sum_{\tilde b=1}^B
    \sum_{\tilde b' \neq \tilde b}
    \E \Bigg[
      \varepsilon_i^2
      \varepsilon_j^2 \\
      &\qquad\times
      \left(
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) }
        {N_{b r}(x) N_{b' r'}(x)}
        - \E
        \left[
          \frac{\I_{i b r}(x) \I_{i b' r'}(x) }
          {N_{b r}(x) N_{b' r'}(x)}
          \Bigm| \bX
        \right]
      \right) \\
      &\qquad
      \times
      \left(
        \frac{\I_{j \tilde b r}(x) \I_{j \tilde b' r'}(x) }
        {N_{\tilde b r}(x) N_{ \tilde b' r'}(x)}
        - \E
        \left[
          \frac{\I_{j \tilde b r}(x) \I_{j \tilde b' r'}(x) }
          {N_{\tilde b r}(x) N_{\tilde b' r'}(x)}
          \Bigm| \bX
        \right]
      \right)
    \Bigg].
  \end{align*}
  %
  Since $T_{b r}$ is independent of $T_{b' r'}$ given
  $\bX, \bY$, the summands are zero
  whenever $\big|\{b, b', \tilde b, \tilde b'\}\big| = 4$.
  Since $\E[ \varepsilon_i^2 \mid X_i]$ is bounded
  and by the Cauchy--Schwarz inequality
  and Lemma~\ref{lem:simple_moment_denominator},
  %
  \begin{align*}
    &\E \left[
      \Var \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    &\quad\lesssim
    \frac{n^2}{\lambda^{2d}}
    \frac{1}{B^3}
    \sum_{b=1}^B
    \sum_{b' \neq b}
    \E \left[
      \left(
        \sum_{i=1}^n
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) }
        {N_{b r}(x) N_{b' r'}(x)}
      \right)^2
    \right]
    \lesssim
    \frac{n^2}{\lambda^{2d}}
    \frac{1}{B}
    \E \left[
      \frac{\I_{b r}(x)}{N_{b r}(x)}
      \frac{\I_{b' r'}(x)}{N_{b' r'}(x)}
    \right]
    \lesssim
    \frac{1}{B}
    \to 0.
  \end{align*}
  %
  For the second term in \eqref{eq:total_variance},
  the random variable inside the variance is a nonlinear
  function of the i.i.d.\ variables $(X_i, \varepsilon_i)$,
  so we apply the Efron--Stein inequality
  \citep{efron1981jackknife}.
  Let $(\tilde X_{i j}, \tilde Y_{i j}) = (X_i, Y_i)$
  if $i \neq j$ and be an
  independent copy of $(X_j, Y_j)$,
  denoted $(\tilde X_j, \tilde Y_j)$, if $i = j$,
  and define $\tilde \varepsilon_{i j} = \tilde Y_{i j} - \mu(\tilde X_{i j})$.
  Write
  $\tilde \I_{i j b r}(x) = \I \big\{ \tilde X_{i j} \in T_{b r}(x) \big\}$
  and
  $\tilde \I_{j b r}(x) = \I \big\{ \tilde X_{j} \in T_{b r}(x) \big\}$,
  and also
  $\tilde N_{j b r}(x) = \sum_{i=1}^{n} \tilde \I_{i j b r}(x)$.
  We use the leave-one-out notation
  $N_{-j b r}(x) = \sum_{i \neq j} \I_{i b r}(x)$
  and also write
  $N_{-j b r \cap b' r'} = \sum_{i \neq j} \I_{i b r}(x) \I_{i b' r'}(x)$.
  Since $\E[ \varepsilon_i^4 \mid X_i]$ is bounded,
  %
  \begin{align*}
    &\Var \left[
      \E \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    &\leq
    \Var \left[
      \E \left[
        \frac{n}{\lambda^d}
        \sum_{i=1}^n
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    &\quad\leq
    \frac{1}{2}
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \sum_{i=1}^n
        \left(
          \frac{\I_{i b r}(x) \I_{i b' r}(x) \varepsilon_i^2}
          {N_{b r}(x) N_{b' r'}(x)}
          - \frac{\tilde \I_{i j b r}(x) \tilde \I_{i j b' r'}(x)
          \tilde \varepsilon_{i j}^2}
          {\tilde N_{j b r}(x) \tilde N_{j b' r'}(x)}
        \right)
      \right)^2
    \right] \\
    &\quad\leq
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \left|
        \frac{1}
        {N_{b }(x) N_{b' r'}(x)}
        - \frac{1}
        {\tilde N_{j b r}(x) \tilde N_{j b' r'}(x)}
        \right|
        \sum_{i \neq j}
        \I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2
      \right)^2
    \right] \\
    &\qquad+
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      \left(
        \left(
          \frac{\I_{j b r}(x) \I_{j b' r'}(x) \varepsilon_j^2}
          {N_{b r}(x) N_{b' r'}(x)}
          - \frac{\tilde \I_{j b r}(x) \tilde \I_{j b' r'}(x)
          \tilde \varepsilon_j^2}
          {\tilde N_{j b r}(x) \tilde N_{j b' r'}(x)}
        \right)
      \right)^2
    \right] \\
    &\quad\lesssim
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      N_{-j b r \cap b' r}(x)^2
      \left|
      \frac{1}
      {N_{b r}(x) N_{b' r'}(x)}
      - \frac{1}
      {\tilde N_{j b r}(x) \tilde N_{j b' r'}(x)}
      \right|^2
      + \frac{\I_{j b r}(x) \I_{j b' r'}(x)}
      {N_{b r}(x)^2 N_{b' r'}(x)^2}
    \right].
  \end{align*}
  %
  For the first term in the above display, note that
  %
  \begin{align*}
    &\left|
    \frac{1}{N_{b r}(x) N_{b' r'}(x)}
    - \frac{1} {\tilde N_{j b r}(x) \tilde N_{j b' r'}(x)}
    \right| \\
    &\quad\leq
    \frac{1}{N_{b r}(x)}
    \left|
    \frac{1} {N_{b' r'}(x)} - \frac{1} {\tilde N_{j b' r'}(x)}
    \right|
    + \frac{1}{\tilde N_{j b' r'}(x)}
    \left|
    \frac{1} {N_{b r}(x)} - \frac{1} {\tilde N_{j b r}(x)}
    \right| \\
    &\quad\leq
    \frac{1}{N_{-j b r}(x)}
    \frac{1} {N_{-j b' r'}(x)^2}
    + \frac{1}{N_{-j b' r'}(x)}
    \frac{1} {N_{-j b r}(x)^2}
  \end{align*}
  %
  since $|N_{b r}(x) - \tilde N_{j b r}(x)| \leq 1$
  and $|N_{b' r'}(x) - \tilde N_{j b' r'}(x)| \leq 1$.
  Further, these terms are non-zero only on the events
  $\{ X_j \in T_{b r}(x) \} \cup \{ \tilde X_j \in T_{b r}(x) \}$
  and
  $\{ X_j \in T_{b' r'}(x) \} \cup \{ \tilde X_j \in T_{b' r'}(x) \}$
  respectively, so
  %
  \begin{align*}
    &\Var \left[
      \E \left[
        \frac{n}{\lambda^d}
        \frac{1}{B^2}
        \sum_{i=1}^n
        \sum_{b=1}^B
        \sum_{b' \neq b}
        \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
        {N_{b r}(x) N_{b' r'}(x)}
        \Bigm| \bX, \bY
      \right]
    \right] \\
    &\, \lesssim
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      \frac{\I_{j b' r'}(x) + \tilde \I_{j b' r'}(x)}{N_{-j b r}(x)^2}
      \frac{N_{-j b r \cap b' r}(x)^2} {N_{-j b' r'}(x)^4}
      \right. \\
      &\left.
      \qquad+
      \frac{\I_{j b r}(x) + \tilde \I_{j b r}(x)}{N_{-j b' r'}(x)^2}
      \frac{N_{-j b r \cap b' r}(x)^2} {N_{-j b r}(x)^4}
      +
      \frac{\I_{j b r}(x) \I_{j b' r'}(x)}
      {N_{b r}(x)^2 N_{b' r'}(x)^2}
    \right] \\
    &\, \lesssim
    \frac{n^2}{\lambda^{2d}}
    \sum_{j=1}^{n}
    \E \left[
      \frac{\I_{j b r}(x) \I_{b r}(x) \I_{b' r'}(x)}
      {N_{b r}(x)^2 N_{b' r'}(x)^2}
    \right]
    \lesssim
    \frac{n^2}{\lambda^{2d}}
    \E \left[
      \frac{\I_{b r}(x) \I_{b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)^2}
    \right] \\
    &\lesssim
    \frac{n^2}{\lambda^{2d}}
    \frac{\lambda^d}{n}
    \frac{\lambda^{2d} \log n}{n^2}
    \lesssim
    \frac{\lambda^d \log n}{n}
    \to 0,
  \end{align*}
  %
  where we applied Lemma~\ref{lem:simple_moment_denominator}.
  So
  $\sum_{i=1}^{n} S_i(x)^2 - n \,\E \left[ S_i(x)^2 \right]
  = O_\P \left( \frac{1}{\sqrt B} + \sqrt{\frac{\lambda^d \log n}{n}} \right)
  = o_\P(1)$.

  \proofparagraph{calculating the limiting variance}
  %
  Thus by \cite[Theorem~3.2]{hall2014martingale}
  we conclude that
  %
  \begin{align*}
    \sqrt{\frac{n}{\lambda^d}}
    \big(
      \hat\mu_\rd(x)
      - \E\left[
        \hat\mu_\rd(x) \mid \bX, \bT
      \right]
    \big)
    &\rightsquigarrow
    \cN\big(0, \Sigma_\rd(x)\big)
  \end{align*}
  %
  as $n \to \infty$, assuming that the limit
  %
  \begin{align*}
    \Sigma_\rd(x)
    &=
    \lim_{n \to \infty}
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
  \end{align*}
  %
  exists. Now we verify this and calculate the limit.
  Since $J$ is fixed, it suffices to find
  %
  \begin{align*}
    \lim_{n \to \infty}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
  \end{align*}
  %
  for each $0 \leq r, r' \leq J$.
  Firstly, note that
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \sigma^2(X_i)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &=
    \frac{n^2}{\lambda^d}
    \sigma^2(x)
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &\quad+
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)
      \big(\sigma^2(X_i) - \sigma^2(x) \big)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right].
  \end{align*}
  %
  Since $\sigma^2$ is Lipschitz and
  $\P \left(\max_{1 \leq l \leq d}
  |T_b(x)_l| \geq t/\lambda \right) \leq 2d e^{-t/2}$
  by Lemma~\ref{lem:largest_cell},
  we have by Lemma~\ref{lem:simple_moment_denominator} that
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)
      \big|\sigma^2(X_i) - \sigma^2(x) \big|}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &\leq
    2de^{-t/2}
    \frac{n^2}{\lambda^d}
    + \frac{n^2}{\lambda^d}
    \frac{t}{\lambda}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &\lesssim
    \frac{n^2}{\lambda^d}
    \frac{\log n}{\lambda}
    \frac{\lambda^d}{n^2}
    \lesssim
    \frac{\log n}{\lambda},
  \end{align*}
  %
  where we set $t = 4 \log n$.
  Therefore
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \sigma^2(x)
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    + O \left( \frac{\log n}{\lambda} \right).
  \end{align*}
  %
  Next, by conditioning on
  $T_{b r}$, $T_{b' r'}$, $N_{-i b r}(x)$, and $N_{-i b' r'}(x)$,
  %
  \begin{align*}
    &\E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &\quad= \E \left[
      \frac{\int_{T_{b r}(x) \cap T_{b' r'}(x)} f(\xi) \diff \xi}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right] \\
    &\quad= f(x) \,
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right]
    +
    \E \left[
      \frac{\int_{T_{b r}(x) \cap T_{b' r'}(x)}
      (f(\xi) - f(x)) \diff \xi}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right] \\
    &\quad=
    f(x) \,
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right]
    + O \left(
      \frac{\lambda^d}{n^2}
      \frac{(\log n)^{d+1}}{\lambda}
    \right)
  \end{align*}
  %
  arguing using Lemma~\ref{lem:largest_cell},
  the Lipschitz property of $f(x)$,
  and Lemma~\ref{lem:simple_moment_denominator}. So
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \sigma^2(x)
    f(x)
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right] \\
    &\quad+
    O \left(
      \frac{(\log n)^{d+1}}{\lambda}
    \right).
  \end{align*}
  %
  Now we apply the binomial result in
  Lemma~\ref{lem:binomial_expectation}
  to approximate the expectation. With
  $N_{-i b' r' \setminus b r}(x) =
  \sum_{j \neq i} \I\{X_j \in T_{b' r'}(x) \setminus T_{b r}(x)\}$,
  %
  \begin{align*}
    &\E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
    \right]
    = \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {N_{-i b r}(x)+1}
      \right. \\
      &\qquad\left.
      \times \,
      \E \left[
        \frac{1}
        {N_{-i b' r' \cap b r}(x)+N_{-i b' r' \setminus b r}(x)+1}
        \Bigm| \bT, N_{-i b' r' \cap b r}(x), N_{-i b r \setminus b' r'}(x)
      \right]
    \right].
  \end{align*}
  %
  Now conditional on
  $\bT$, $N_{-i b' r' \cap b r}(x)$, and $N_{-i b r \setminus b' r'}(x)$,
  %
  \begin{align*}
    N_{-i b' r' \setminus b r}(x)
    &\sim \Bin\left(
      n - 1 - N_{-i b r}(x), \
      \frac{\int_{T_{b' r'}(x) \setminus T_{b r}(x)} f(\xi) \diff \xi}
      {1 - \int_{T_{b r}(x)}
      f(\xi) \diff \xi}
    \right).
  \end{align*}
  %
  Now we bound these parameters above and below.
  Firstly, by applying Lemma~\ref{lem:active_data} with $B=1$,
  we have
  %
  \begin{align*}
    \P \left( N_{-i b r}(x) >
      t^{d+1}
      \frac{n}{\lambda^d}
    \right)
    &\leq
    4 d e^{- t / (4 \|f\|_\infty(1 + 1/a_r))}
    \leq
    e^{- t / C}
  \end{align*}
  %
  for some $C > 0$ and all sufficiently large $t$.
  Next, note that if $f$ is $L$-Lipschitz in $\ell^2$,
  by Lemma~\ref{lem:largest_cell}
  %
  \begin{align*}
    &\P \left(
      \left|
      \frac{\int_{T_{b' r'}(x) \setminus T_{b r}(x)} f(\xi) \diff \xi}
      {1 - \int_{T_{b r}(x)} f(\xi)
      \diff \xi}
      - f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|
      \right|
      > t \, \frac{|T_{b' r'}(x) \setminus T_{b r}(x)|}{\lambda}
    \right) \\
    &\quad\leq
    \P \left(
      \int_{T_{b' r'}(x) \setminus T_{b r}(x)}
      \left| f(\xi) - f(x) \right|
      \diff \xi
      > t \, \frac{|T_{b' r'}(x) \setminus T_{b r}(x)|}{2 \lambda}
    \right) \\
    &\qquad+
    \P \left(
      \frac{\int_{T_{b' r'}(x) \setminus T_{b r}(x)} f(\xi) \diff \xi
      \cdot \int_{T_{b r}(x)} f(\xi) \diff \xi}
      {1 - \int_{T_{b r}(x)} f(\xi) \diff \xi}
      > t \, \frac{|T_{b' r'}(x) \setminus T_{b r}(x)|}{2\lambda}
    \right) \\
    &\quad\leq
    \P \left(
      L d\,
      |T_{b' r'}(x) \setminus T_{b r}(x)|
      \max_{1 \leq j \leq d} |T_{b' r'}(x)_j|
      > t \, \frac{|T_{b' r'}(x) \setminus T_{b r}(x)|}{2\lambda}
    \right) \\
    &\qquad+
    \P \left(
      \|f\|_\infty
      \,|T_{b' r'}(x) \setminus T_{b r}(x)|
      \frac{\|f\|_\infty |T_{b r}(x)|}
      {1 - \|f\|_\infty |T_{b r}(x)|}
      > t \, \frac{|T_{b' r'}(x) \setminus T_{b r}(x)|}{2\lambda}
    \right) \\
    &\quad\leq
    \P \left(
      \max_{1 \leq j \leq d} |T_{b' r'}(x)_j|
      >  \frac{t}{2\lambda L d}
    \right)
    +\P \left(
      |T_{b r}(x)|
      > \frac{t}{4\lambda \|f\|_\infty^2}
    \right) \\
    &\quad\leq
    2 d e^{-t a_r /(4L d)}
    + 2 d e^{-t a_r / (8 \|f\|_\infty^2)}
    \leq e^{-t/C},
  \end{align*}
  %
  for large $t$,
  increasing $C$ as necessary.
  Thus with probability at least $1 - e^{-t/C}$,
  again increasing $C$,
  %
  \begin{align*}
    N_{-i b' r' \setminus b r}(x)
    &\leq \Bin\left(
      n, \,
      |T_{b' r'}(x) \setminus T_{b r}(x)|
      \left( f(x) + \frac{t}{\lambda} \right)
    \right) \\
    N_{-i b' r' \setminus b r}(x)
    &\geq
    \Bin\left(
      n
      \left( 1 - \frac{t^{d+1}}{\lambda^d}
      - \frac{1}{n} \right), \,
      |T_{b' r'}(x) \setminus T_{b r}(x)|
      \left( f(x) - \frac{t}{\lambda} \right)
    \right).
  \end{align*}
  %
  So by Lemma~\ref{lem:binomial_expectation} conditionally on
  $\bT$, $N_{-i b' r' \cap b r}(x)$, and $N_{-i b r \setminus b' r'}(x)$,
  with probability at least $1 - e^{-t/C}$,
  %
  \begin{align*}
    &\left|
    \E \left[
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+N_{-i b' r' \setminus b r}(x)+1}
      \Bigm| \bT, N_{-i b' r' \cap b r}(x), N_{-i b r \setminus b' r'}(x)
    \right]
    \right.
    \\
    &\left.
    \qquad-
    \frac{1}
    {N_{-i b' r' \cap b r}(x) + n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right| \\
    &\quad\lesssim
    \frac{1 + \frac{n t}{\lambda} |T_{b' r'}(x) \setminus T_{b r}(x)|}
    {\left(N_{-i b' r' \cap b r}(x)
    + n |T_{b' r'}(x) \setminus T_{b r}(x)|+1\right)^2}.
  \end{align*}
  %
  Therefore by the same approach as the proof of
  Lemma~\ref{lem:moment_denominator},
  taking $t = 3 C \log n$,
  %
  \begin{align*}
    &
    \left|
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1) (N_{-i b' r'}(x)+1)}
      \right.\right. \\
      &\left.\left.
      \qquad -
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(N_{-i b r}(x)+1)
        (N_{-i b' r' \cap b r}(x)+n f(x)
      |T_{b' r'}(x) \setminus T_{b r}(x)|+1)}
    \right]
    \right| \\
    &\quad\lesssim
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}{N_{-i b r}(x)+1}
      \frac{1 + \frac{n t}{\lambda} |T_{b' r'}(x) \setminus T_{b r}(x)|}
      {\left(N_{-i b' r' \cap b r}(x)
      + n |T_{b' r'}(x) \setminus T_{b r}(x)|+1\right)^2}
    \right]
    +
    e^{-t/C} \\
    &\quad\lesssim
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {n |T_{b r}(x)|+1}
      \frac{1 + \frac{n t}{\lambda} |T_{b' r'}(x) \setminus T_{b r}(x)|}
      {(n |T_{b' r'}(x)| + 1)^2}
    \right]
    + e^{-t/C} \\
    &\quad\lesssim
    \E \left[
      \frac{1}{n}
      \frac{1}
      {(n |T_{b' r'}(x)| + 1)^2}
      + \frac{1}{n}
      \frac{t / \lambda}
      {n |T_{b' r'}(x)| + 1}
    \right]
    + e^{-t/C} \\
    &\quad\lesssim
    \frac{\lambda^{2d} \log n}{n^3}
    + \frac{\log n}{n \lambda}
    \frac{\lambda^d}{n}
    \lesssim
    \frac{\lambda^d}{n^2}
    \left(
      \frac{\lambda^{d} \log n}{n}
      + \frac{\log n}{\lambda}
    \right).
  \end{align*}
  %
  Now apply the same argument to the other
  term in the expectation, to see that
  %
  \begin{align*}
    &\left|
    \E \left[
      \frac{1}
      {N_{-i b r \cap b' r'}(x)+N_{-i b r \setminus b' r'}(x)+1}
      \Bigm| \bT, N_{-i b r \cap b' r'}(x), N_{-i b' r' \setminus b r}(x)
    \right]
    \right. \\
    &\left.
    \qquad-
    \frac{1}
    {N_{-i b r \cap b' r'}(x) + n f(x) |T_{b r}(x) \setminus T_{b' r'}(x)|+1}
    \right| \\
    &\quad\lesssim
    \frac{1 + \frac{n t}{\lambda} |T_{b r}(x) \setminus T_{b' r'}(x)|}
    {\left(N_{-i b r \cap b' r'}(x)
    + n |T_{b r}(x) \setminus T_{b' r'}(x)|+1\right)^2}.
  \end{align*}
  %
  with probability at least $1 - e^{-t/C}$,
  and so likewise again with $t = 3 C \log n$,
  %
  \begin{align*}
    &\frac{n^2}{\lambda^d}
    \left|
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}{N_{-i b r}(x)+1}
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right]
    \right.
    \\
    &\left.
    \quad-
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {N_{-i b r \cap b' r'}(x) + n f(x) |T_{b r}(x) \setminus T_{b' r'}(x)|+1}
      \right.\right. \\
      &\qquad\qquad\left.\left.
      \times
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right]
    \right| \\
    &\lesssim
    \frac{n^2}{\lambda^d} \,
    \E \left[
      \frac{1 + \frac{n t}{\lambda} |T_{b r}(x) \setminus T_{b' r'}(x)|}
      {\left(N_{-i b r \cap b' r'}(x)
      + n |T_{b r}(x) \setminus T_{b' r'}(x)|+1\right)^2}
      \right. \\
      &\qquad\qquad\left.
      \times
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right]
    + \frac{n^2}{\lambda^d}
    e^{-t/C} \\
    &\lesssim
    \frac{\lambda^d \log n}{n}
    + \frac{\log n}{\lambda}.
  \end{align*}
  %
  Thus far we have proven that
  %
  \begin{align*}
    &\frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    = \sigma^2(x)
    f(x)
    \frac{n^2}{\lambda^d} \\
    &\quad\times
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {N_{-i b r \cap b' r'}(x) + n f(x) |T_{b r}(x) \setminus T_{b' r'}(x)|+1}
      \right. \\
      &\left.
      \qquad\qquad
      \times
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right] \\
    &\quad+
    O \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{\lambda^d \log n}{n}
    \right).
  \end{align*}
  %
  Next we remove the $N_{-i b r \cap b' r'}(x)$ terms.
  As before, with probability at least $1 - e^{-t/C}$,
  conditional on $\bT$,
  %
  \begin{align*}
    N_{-i b r \cap b' r'}(x)
    &\leq \Bin\left(
      n, \,
      |T_{b r}(x) \cap T_{b' r'}(x)|
      \left( f(x) + \frac{t}{\lambda} \right)
    \right), \\
    N_{-i b r \cap b' r'}(x)
    &\geq
    \Bin\left(
      n
      \left( 1 - \frac{t^{d+1}}{\lambda^d}
      - \frac{1}{n} \right), \,
      |T_{b r}(x) \cap T_{b' r'}(x)|
      \left( f(x) - \frac{t}{\lambda} \right)
    \right).
  \end{align*}
  %
  Therefore by Lemma~\ref{lem:binomial_expectation}
  applied conditionally on $\bT$,
  with probability at least $1 - e^{-t/C}$,
  %
  \begin{align*}
    &
    \left|
    \E \left[
      \frac{1}
      {N_{-i b r \cap b' r'}(x) + n f(x) |T_{b r}(x) \setminus T_{b' r'}(x)|+1}
      \right.\right. \\
      &\quad\qquad\left.\left.
      \times
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
      \Bigm| \bT
    \right]
    \right.
    \\
    &\left.
    \qquad-
    \frac{1}
    {n f(x) |T_{b r}(x)|+1}
    \frac{1}
    {n f(x) |T_{b' r'}(x)|+1}
    \right| \\
    &\quad\lesssim
    \frac{1 + \frac{n t}{\lambda} |T_{b r}(x) \cap T_{b' r'}(x)|}
    {(n |T_{b r}(x)| + 1)(n |T_{b' r'}(x)| + 1)}
    \left(
      \frac{1}{n |T_{b r}(x)| + 1}
      + \frac{1}{n |T_{b' r'}(x)| + 1}
    \right).
  \end{align*}
  %
  Now by Lemma~\ref{lem:moment_cell},
  with $t = 3 C \log n$,
  %
  \begin{align*}
    &\frac{n^2}{\lambda^d}
    \left|
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {N_{-i b r \cap b' r'}(x) + n f(x) |T_{b r}(x) \setminus T_{b' r'}(x)|+1}
      \right.\right. \\
      &\quad\qquad\left.\left.
      \times
      \frac{1}
      {N_{-i b' r' \cap b r}(x)+n f(x) |T_{b' r'}(x) \setminus T_{b r}(x)|+1}
    \right]
    \right. \\
    &\left.
    \qquad-
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {n f(x) |T_{b r}(x)|+1}
      \frac{1}
      {n f(x) |T_{b' r'}(x)|+1}
    \right]
    \right| \\
    &\quad\lesssim
    \frac{n^2}{\lambda^d}
    \E \left[
      |T_{b r}(x) \cap T_{b' r'}(x)|
      \frac{1 + \frac{n t}{\lambda} |T_{b r}(x) \cap T_{b' r'}(x)|}
      {(n |T_{b r}(x)| + 1)(n |T_{b' r'}(x)| + 1)}
      \right.\\
      &\quad\qquad\qquad\left.
      \times
      \left(
        \frac{1}{n |T_{b r}(x)| + 1}
        + \frac{1}{n |T_{b' r'}(x)| + 1}
      \right)
    \right] 
    + \frac{n^2}{\lambda^d}
    e^{-t/C} \\
    &\quad\lesssim
    \frac{n^2}{\lambda^d}
    \frac{1}{n^3}
    \E \left[
      \frac{1 + \frac{n t}{\lambda} |T_{b r}(x) \cap T_{b' r'}(x)|}
      {|T_{b r}(x)| |T_{b' r'}(x)|}
    \right]
    + \frac{n^2}{\lambda^d}
    e^{-t/C} \\
    &\quad\lesssim
    \frac{1}{n \lambda^d}
    \E \left[
      \frac{1}{|T_{b r}(x)| |T_{b' r'}(x)|}
    \right]
    + \frac{t}{\lambda^{d+1}}
    \E \left[
      \frac{1}{|T_{b r}(x)|}
    \right]
    + \frac{n^2}{\lambda^d}
    e^{-t/C} \\
    &\quad\lesssim
    \frac{\lambda^d}{n}
    + \frac{\log n}{\lambda}.
  \end{align*}
  %
  This allows us to deduce that
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \sigma^2(x)
    f(x)
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {(n f(x) |T_{b r}(x)|+1)(n f(x) |T_{b' r'}(x)|+1)}
    \right] \\
    &\quad+
    O \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{\lambda^d \log n}{n}
    \right).
  \end{align*}
  %
  Now that we have reduced the limiting variance to an expression
  only involving the sizes of Mondrian cells,
  we can exploit their exact distribution to compute this expectation.
  Recall from \citet[Proposition~1]{mourtada2020minimax}
  that we can write
  %
  \begin{align*}
    |T_{b r}(x)|
    &= \prod_{j=1}^{d}
    \left(
      \frac{E_{1j}}{a_r \lambda} \wedge x_j
      + \frac{E_{2j}}{a_r \lambda} \wedge (1 - x_j)
    \right), \\
    |T_{b' r'}(x)|
    &=
    \prod_{j=1}^{d}
    \left(
      \frac{E_{3j}}{a_{r'} \lambda} \wedge x_j
      +  \frac{E_{4j}}{a_{r'} \lambda} \wedge (1 - x_j)
    \right), \\
    |T_{b r }(x)\cap T_{b' r'}(x)|
    &= \prod_{j=1}^{d}
    \left(
      \frac{E_{1j}}{a_r \lambda} \wedge
      \frac{E_{3j}}{a_{r'} \lambda}
      \wedge x_j
      +  \frac{E_{2j}}{a_r \lambda} \wedge
      \frac{E_{4j}}{a_{r'} \lambda}
      \wedge (1 - x_j)
    \right)
  \end{align*}
  %
  where $E_{1j}$, $E_{2j}$, $E_{3j}$, and $E_{4j}$
  are independent and $\Exp(1)$.
  Define their non-truncated versions as
  %
  \begin{align*}
    |\tilde T_{b r}(x)|
    &=
    a_r^{-d}
    \lambda^{-d}
    \prod_{j=1}^{d}
    \left( E_{1j} + E_{2j} \right), \\
    |\tilde T_{b' r'}(x)|
    &=
    a_{r'}^{-d}
    \lambda^{-d}
    \prod_{j=1}^{d}
    \left( E_{3j} + E_{4j} \right), \\
    |\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|
    &=
    \lambda^{-d}
    \prod_{j=1}^{d}
    \left(
      \frac{E_{1j}}{a_r}
      \wedge
      \frac{E_{3j}}{a_{r'}}
      + \frac{E_{2j}}{a_r}
      \wedge
      \frac{E_{4j}}{a_{r'}}
    \right),
  \end{align*}
  %
  and note that
  %
  \begin{align*}
    &\P \left(
      \big( \tilde T_{b r}(x), \tilde T_{b' r'}(x),
      \tilde T_{b r}(x) \cap T_{b' r'}(x) \big)
      \neq
      \big( T_{b r}(x), T_{b' r'}(x), T_{b r}(x) \cap T_{b' r'}(x) \big)
    \right) \\
    &\quad\leq
    \sum_{j=1}^{d}
    \big(
      \P(E_{1j} \geq a_r \lambda x_j)
      + \P(E_{3j} \geq a_{r'} \lambda x_j) \\
      &\qquad\qquad+
      \P(E_{2j} \geq a_r \lambda (1 - x_j))
      + \P(E_{4j} \geq a_{r'} \lambda (1 - x_j))
    \big) \\
    &\quad\leq
    e^{-C \lambda}
  \end{align*}
  %
  for some $C > 0$ and sufficiently large $\lambda$.
  Hence by the Cauchy--Schwarz inequality
  and Lemma~\ref{lem:moment_cell},
  %
  \begin{align*}
    &
    \frac{n^2}{\lambda^d}
    \left|
    \E \left[
      \frac{|T_{b r}(x) \cap T_{b' r'}(x)|}
      {n f(x) |T_{b r}(x)|+1}
      \frac{1}
      {n f(x) |T_{b' r'}(x)|+1}
    \right]
    - \E \left[
      \frac{|\tilde T_{b r}(x) \cap T_{b' r'}(x)|}
      {n f(x) |\tilde T_{b r}(x)|+1}
      \frac{1}
      {n f(x) |\tilde T_{b' r'}(x)|+1}
    \right]
    \right| \\
    &\quad\lesssim
    \frac{n^2}{\lambda^d}
    e^{-C \lambda}
    \lesssim
    e^{-C \lambda / 2}
  \end{align*}
  %
  as $\log \lambda \gtrsim \log n$.
  Therefore
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \sigma^2(x)
    f(x)
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {(n f(x) |\tilde T_{b r}(x)|+1)(n f(x) |\tilde T_{b' r'}(x)|+1)}
    \right] \\
    &\quad+
    O \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{\lambda^d \log n}{n}
    \right).
  \end{align*}
  %
  Now we remove the superfluous units in the denominators.
  Firstly, by independence of the trees,
  %
  \begin{align*}
    & \frac{n^2}{\lambda^d}
    \left|
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {(n f(x) |\tilde T_{b r}(x)|+1)(n f(x) |\tilde T_{b' r'}(x)|+1)}
    \right]
    \right .\\
    &\left.
    \qquad-
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {(n f(x) |\tilde T_{b r}(x)|+1)(n f(x) |\tilde T_{b' r'}(x)|)}
    \right]
    \right| \\
    &\quad\lesssim
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {n |\tilde T_{b r}(x)|}
      \frac{1}
      {n^2 |\tilde T_{b' r'}(x)|^2}
    \right] \\
    &\quad\lesssim
    \frac{1}{n \lambda^d}
    \E \left[
      \frac{1}{|T_{b r}(x)|}
    \right]
    \E \left[
      \frac{1}{|T_{b' r'}(x)|}
    \right]
    \lesssim
    \frac{\lambda^d}{n}.
  \end{align*}
  %
  Secondly, we have in exactly the same manner that
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \left|
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap T_{b' r'}(x)|}
      {(n f(x) |\tilde T_{b r}(x)|+1)(n f(x) |\tilde T_{b' r'}(x)|)}
    \right]
    - \E \left[
      \frac{|\tilde T_{b r}(x) \cap T_{b' r'}(x)|}
      {n^2 f(x)^2 |\tilde T_{b r}(x)| |\tilde T_{b' r'}(x)|}
    \right]
    \right|
    &\lesssim
    \frac{\lambda^d}{n}.
  \end{align*}
  %
  Therefore
  %
  \begin{align*}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right]
    &=
    \frac{\sigma^2(x)}{f(x)}
    \frac{1}{\lambda^d}
    \E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {|\tilde T_{b r}(x)| |\tilde T_{b' r'}(x)|}
    \right]
    + O \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{\lambda^d \log n}{n}
    \right).
  \end{align*}
  %
  It remains to compute this integral.
  By independence over $1 \leq j \leq d$,
  %
  \begin{align*}
    &\E \left[
      \frac{|\tilde T_{b r}(x) \cap \tilde T_{b' r'}(x)|}
      {|\tilde T_{b r}(x)| |\tilde T_{b' r'}(x)|}
    \right] \\
    &\quad=
    a_r^d a_{r'}^d \lambda^d
    \prod_{j=1}^d
    \E \left[
      \frac{ (E_{1j} / a_r) \wedge (E_{3j} / a_{r'})
      + (E_{2j} a_r) \wedge (E_{4j} / a_{r'}) }
      { \left( E_{1j} + E_{2j} \right) \left( E_{3j} + E_{4j} \right)}
    \right] \\
    &\quad=
    2^d a_r^d a_{r'}^d \lambda^d
    \prod_{j=1}^d
    \E \left[
      \frac{ (E_{1j} / a_r) \wedge (E_{3j} / a_{r'})}
      { \left( E_{1j} + E_{2j} \right) \left( E_{3j} + E_{4j} \right) }
    \right] \\
    &\quad=
    2^d a_r^d a_{r'}^d \lambda^d
    \prod_{j=1}^d
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    \frac{ (t_1 / a_r) \wedge (t_3 / a_{r'}) }
    { \left( t_1 + t_2 \right) \left( t_3 + t_4 \right) }
    e^{-t_1 - t_2 - t_3 - t_4}
    \diff t_1
    \diff t_2
    \diff t_3
    \diff t_4 \\
    &\quad=
    2^d a_r^d a_{r'}^d \lambda^d
    \prod_{j=1}^d
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    ((t_1 / a_r) \wedge (t_3 / a_{r'}))
    e^{-t_1 - t_3} \\
    &\qquad\times
    \left(
      \int_{0}^{\infty}
      \frac{e^{-t_2}}{t_1 + t_2}
      \diff t_2
    \right)
    \left(
      \int_{0}^{\infty}
      \frac{e^{-t_4}}{t_3 + t_4}
      \diff t_4
    \right)
    \diff t_1
    \diff t_3 \\
    &\quad=
    2^d a_r^d a_{r'}^d \lambda^d
    \prod_{j=1}^d
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    ((t / a_r) \wedge (s / a_{r'}))
    \Gamma(0, t)
    \Gamma(0, s)
    \diff t
    \diff s,
  \end{align*}
  %
  where we used
  $\int_0^\infty \frac{e^{-t}}{a + t} \diff t = e^a \Gamma(0, a)$
  with $\Gamma(0, a) = \int_a^\infty \frac{e^{-t}}{t} \diff t$
  the upper incomplete gamma function. Now
  %
  \begin{align*}
    &2
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    ((t / a_r) \wedge (s / a_{r'}))
    \Gamma(0, t)
    \Gamma(0, s)
    \diff t
    \diff s \\
    &\quad=
    \int_0^\infty
    \Gamma(0, t)
    \left(
      \frac{1}{a_{r'}}
      \int_0^{a_{r'} t / a_r}
      2 s \Gamma(0, s)
      \diff{s}
      +
      \frac{t}{a_r}
      \int_{a_{r'} t / a_r}^\infty
      2 \Gamma(0, s)
      \diff{s}
    \right)
    \diff{t} \\
    &\quad=
    \int_0^\infty
    \Gamma(0, t)
    \left(
      \frac{t}{a_r}
      e^{- \frac{a_{r'}}{a_r}t}
      - \frac{1}{a_{r'}} e^{- \frac{a_{r'}}{a_r}t}
      + \frac{1}{a_{r'}}
      - \frac{a_{r'}}{a_r^2} t^2
      \Gamma\left(0, \frac{a_{r'}}{a_r} t\right)
    \right)
    \diff{t} \\
    &\quad=
    \frac{1}{a_r}
    \int_0^\infty
    t e^{- \frac{a_{r'}}{a_r} t}
    \Gamma(0, t)
    \diff{t}
    - \frac{1}{a_{r'}}
    \int_0^\infty
    e^{- \frac{a_{r'}}{a_r} t}
    \Gamma(0, t)
    \diff{t} \\
    &\qquad+
    \frac{1}{a_{r'}}
    \int_0^\infty
    \Gamma(0, t)
    \diff{t}
    -
    \frac{a_{r'}}{a_r^2}
    \int_0^\infty
    t^2 \Gamma\left(0, \frac{a_{r'}}{a_r} t\right)
    \Gamma(0, t)
    \diff{t},
  \end{align*}
  %
  since
  $\int_0^a 2 t \Gamma(0, t) \diff t = a^2 \Gamma(0, a) - a e^{-a} -e^{-a} + 1$
  and
  $\int_a^\infty \Gamma(0, t) \diff t = e^{-a} - a \Gamma(0, a)$.
  Next, we use
  %
  $ \int_{0}^{\infty} \Gamma(0, t) \diff t = 1$,
  $\int_{0}^{\infty} e^{-at} \Gamma(0, t) \diff t
  = \frac{\log(1+a)}{a}$,
  $\int_{0}^{\infty} t e^{-at} \Gamma(0, t) \diff t
  = \frac{\log(1+a)}{a^2} - \frac{1}{a(a+1)}$
  and
  $\int_{0}^{\infty} t^2 \Gamma(0, t) \Gamma(0, at) \diff t
  = - \frac{2a^2 + a + 2}{3a^2 (a+1)} + \frac{2(a^3 + 1) \log(a+1)}{3a^3}
  - \frac{2 \log a}{3}$
  to see
  %
  \begin{align*}
    &2
    \int_{0}^{\infty}
    \int_{0}^{\infty}
    ((t / a_r) \wedge (s / a_{r'}))
    \Gamma(0, t)
    \Gamma(0, s)
    \diff t
    \diff s \\
    &\quad=
    \frac{a_r \log(1+a_{r'} / a_r)}{a_{r'}^2}
    - \frac{a_r / a_{r'}}{a_r + a_{r'}}
    - \frac{a_r \log(1 + a_{r'} / a_r)}{a_{r'}^2}
    + \frac{1}{a_{r'}} \\
    &\qquad+
    \frac{2 a_{r'}^2 + a_r a_{r'} + 2 a_r^2}
    {3 a_r a_{r'} (a_r + a_{r'})}
    - \frac{2(a_{r'}^3 + a_r^3) \log(a_{r'} / a_r+1)}{3 a_r^2 a_{r'}^2}
    + \frac{2 a_{r'} \log (a_{r'} / a_r)}{3 a_r^2} \\
    &\quad=
    \frac{2}{3 a_r} + \frac{2}{3 a_{r'}}
    - \frac{2(a_r^3 + a_{r'}^3 ) \log(a_{r'} / a_{r}+1)}
    {3 a_r^2 a_{r'}^2}
    + \frac{2 a_{r'} \log (a_{r'} / a_{r})}{3 a_r^2} \\
    &\quad=
    \frac{2}{3 a_r}
    + \frac{2}{3 a_{r'}}
    - \frac{2 a_{r'} \log(a_{r} / a_{r'} + 1)}{3 a_r^2}
    - \frac{2 a_r \log(a_{r'} / a_{r} + 1)}{3 a_{r'}^2} \\
    &\quad=
    \frac{2}{3 a_r}
    \left(
      1 - \frac{a_{r'}}{a_r}
      \log\left(\frac{a_{r}}{a_{r'}} + 1\right)
    \right)
    + \frac{2}{3 a_{r'}}
    \left(
      1 - \frac{a_r }{a_{r'}}
      \log\left(\frac{a_{r'}}{a_{r}} + 1\right)
    \right).
  \end{align*}
  %
  Finally we conclude by giving the limiting variance.
  %
  \begin{align*}
    &\sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'}
    \frac{n^2}{\lambda^d}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x) \varepsilon_i^2}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &\quad=
    \frac{\sigma^2(x)}{f(x)}
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'} \\
    &\qquad\times
    \left(
      \frac{2 a_{r'}}{3}
      \left(
        1 - \frac{a_{r'}}{a_r}
        \log\left(\frac{a_r}{a_{r'}} + 1\right)
      \right)
      + \frac{2 a_r}{3}
      \left(
        1 - \frac{a_r}{a_{r'}}
        \log\left(\frac{a_{r'}}{a_r} + 1\right)
      \right)
    \right)^d \\
    &\qquad+
    O \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{\lambda^d \log n}{n}
    \right).
  \end{align*}
  %
  So the limit exists and
  %
  \begin{align*}
    \Sigma_\rd(x)
    &=
    \frac{\sigma^2(x)}{f(x)}
    \sum_{r=0}^{J}
    \sum_{r'=0}^{J}
    \omega_r
    \omega_{r'} \\
    &\qquad\times
    \left(
      \frac{2 a_r}{3}
      \left(
        1 - \frac{a_r}{a_{r'}}
        \log\left(\frac{a_{r'}}{a_r} + 1\right)
      \right)
      + \frac{2 a_{r'}}{3}
      \left(
        1 - \frac{a_{r'}}{a_r}
        \log\left(\frac{a_r}{a_{r'}} + 1\right)
      \right)
    \right)^d.
  \end{align*}
  %
\end{proof}

\subsection*{Proof of bias characterization with debiasing}

The new bias characterization with debiasing is a purely algebraic
consequence of the original bias characterization and the construction
of the debiased Mondrian random forest estimator.

\begin{proof}[Theorem~\ref{thm:bias_debiased}]

  By the definition of the debiased estimator and Theorem~\ref{thm:bias},
  since $J$ and $a_r$ are fixed,
  %
  \begin{align*}
    \E \big[ \hat \mu_\rd(x) \mid \bX, \bT \big]
    &=
    \sum_{l=0}^J
    \omega_l
    \E \big[
      \hat \mu_l(x)
      \Bigm| \bX, \bT
    \big] \\
    &=
    \sum_{l=0}^J
    \omega_l
    \left(
      \mu(x)
      + \sum_{r=1}^{\lfloor \flbeta / 2 \rfloor}
      \frac{B_r(x)}{a_l^{2r} \lambda^{2r}}
    \right)
    + O_\P \left(
      \frac{1}{\lambda^\beta}
      + \frac{1}{\lambda \sqrt B}
      + \frac{\log n}{\lambda} \sqrt{\frac{\lambda^d}{n}}
    \right).
  \end{align*}
  %
  It remains to evaluate the first term.
  Recalling that $A_{r s} = a_{r-1}^{2 - 2s}$
  and $A \omega = e_0$, we have
  %
  \begin{align*}
    &\sum_{l=0}^J
    \omega_l
    \left(
      \mu(x)
      + \sum_{r=1}^{\lfloor \flbeta / 2 \rfloor}
      \frac{B_r(x)}{a_l^{2r} \lambda^{2r}}
    \right) \\
    &\quad=
    \mu(x)
    \sum_{l=0}^J
    \omega_l
    +
    \sum_{r=1}^{\lfloor \flbeta / 2 \rfloor}
    \frac{B_r(x)}{\lambda^{2r}}
    \sum_{l=0}^J
    \frac{\omega_l}{a_l^{2r}} \\
    &\quad=
    \mu(x)
    (A \omega)_1
    + \sum_{r=1}^{\lfloor \flbeta / 2 \rfloor \wedge J}
    \frac{B_r(x)}{\lambda^{2r}}
    (A \omega)_{r+1}
    + \sum_{r = (\lfloor \flbeta / 2 \rfloor \wedge J) + 1}
    ^{\lfloor \flbeta / 2 \rfloor}
    \frac{B_r(x)}{\lambda^{2r}}
    \sum_{l=0}^J
    \frac{\omega_l}{a_l^{2r}} \\
    &\quad=
    \mu(x)
    + \I\{\lfloor \flbeta / 2 \rfloor \geq J + 1\}
    \frac{B_{J+1}(x)}{\lambda^{2J + 2}}
    \sum_{l=0}^J
    \frac{\omega_l}{a_l^{2J + 2}}
    + O \left( \frac{1}{\lambda^{2J + 4}} \right) \\
    &\quad=
    \mu(x)
    + \I\{2J + 2 < \beta\}
    \frac{\bar\omega B_{J+1}(x)}{\lambda^{2J + 2}}
    + O \left( \frac{1}{\lambda^{2J + 4}} \right).
  \end{align*}
  %
\end{proof}

\subsection*{Proof of variance estimator consistency with debiasing}

\begin{proof}[Theorem~\ref{thm:variance_estimation_debiased}]

  \proofparagraph{consistency of $\hat\sigma^2(x)$}

  Recall that
  %
  \begin{align}
    \label{eq:sigma2_hat_proof}
    \hat\sigma^2(x)
    &=
    \frac{1}{B}
    \sum_{b=1}^{B}
    \frac{\sum_{i=1}^n Y_i^2 \, \I\{X_i \in T_b(x)\}}
    {\sum_{i=1}^n \I\{X_i \in T_b(x)\}}
    - \hat \mu(x)^2.
  \end{align}
  %
  The first term in \eqref{eq:sigma2_hat_proof}
  is simply a Mondrian forest estimator of
  $\E[Y_i^2 \mid X_i = x] = \sigma^2(x) + \mu(x)^2$,
  which is bounded and Lipschitz,
  where $\E[Y_i^4 \mid X_i]$ is bounded almost surely.
  So its conditional bias is controlled
  by Theorem~\ref{thm:bias} and is at most
  $O_\P \left( \frac{1}{\lambda} +
  \frac{\log n}{\lambda} \sqrt{\lambda^d / n} \right)$.
  Its variance is
  at most $\frac{\lambda^d}{n}$ by Theorem~\ref{thm:clt_debiased}.
  Consistency of the second term in \eqref{eq:sigma2_hat_proof}
  follows directly from Theorems~\ref{thm:bias} and \ref{thm:clt_debiased}
  with the same bias and variance bounds.
  Therefore
  %
  \begin{align*}
    \hat\sigma^2(x)
    &=
    \sigma^2(x)
    + O_\P \left(
      \frac{1}{\lambda}
      + \sqrt{\frac{\lambda^d}{n}}
    \right).
  \end{align*}

  \proofparagraph{consistency of the sum}
  %
  Note that
  %
  \begin{align*}
    &\frac{n}{\lambda^d}
    \sum_{i=1}^n
    \left(
      \sum_{r=0}^J
      \omega_r
      \frac{1}{B}
      \sum_{b=1}^B
      \frac{\I\{X_i \in T_{r b}(x)\}}
      {\sum_{i=1}^n \I\{X_i \in T_{r b}(x)\}}
    \right)^2 \\
    &\quad=
    \frac{n}{\lambda^d}
    \frac{1}{B^2}
    \sum_{i=1}^n
    \sum_{r=0}^J
    \sum_{r'=0}^J
    \omega_r
    \omega_{r'}
    \sum_{b=1}^B
    \sum_{b'=1}^B
    \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
    {N_{b r}(x) N_{b' r'}(x)}.
  \end{align*}
  %
  This is exactly the same as the quantity in
  \eqref{eq:clt_condition_sum}, if we were to take
  $\varepsilon_i$ to be $\pm 1$ with equal probability.
  Thus we immediately have convergence in probability
  by the proof of Theorem~\ref{thm:clt_debiased}:
  %
  \begin{align*}
    \frac{n}{\lambda^d}
    \sum_{i=1}^n
    \left(
      \sum_{r=0}^J
      \omega_r
      \frac{1}{B}
      \sum_{b=1}^B
      \frac{\I\{X_i \in T_{r b}(x)\}}
      {\sum_{i=1}^n \I\{X_i \in T_{r b}(x)\}}
    \right)^2
    &=
    \frac{n^2}{\lambda^d}
    \sum_{r=0}^J
    \sum_{r'=0}^J
    \omega_r
    \omega_{r'}
    \E \left[
      \frac{\I_{i b r}(x) \I_{i b' r'}(x)}
      {N_{b r}(x) N_{b' r'}(x)}
    \right] \\
    &\quad+
    O_\P \left(
      \frac{1}{\sqrt B}
      + \sqrt{\frac{\lambda^d \log n}{n}}
    \right).
  \end{align*}

  \proofparagraph{conclusion}

  Again by the proof of Theorem~\ref{thm:clt_debiased}
  with $\varepsilon_i$ being $\pm 1$ with equal probability,
  and by the previous parts,
  %
  \begin{align*}
    \hat\Sigma_\rd(x)
    = \Sigma_\rd(x)
    + O_\P \left(
      \frac{(\log n)^{d+1}}{\lambda}
      + \frac{1}{\sqrt B}
      + \sqrt{\frac{\lambda^d \log n}{n}}
    \right).
  \end{align*}

\end{proof}

\subsection*{Proof of confidence interval validity with debiasing}

\begin{proof}[Theorem~\ref{thm:confidence_debiased}]
  %
  By Theorem~\ref{thm:bias_debiased}
  and Theorem~\ref{thm:variance_estimation_debiased},
  %
  \begin{align*}
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu_\rd(x) - \mu(x)}{\hat \Sigma_\rd(x)^{1/2}}
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu_\rd(x) - \E \left[ \hat \mu_\rd(x) \mid \bX, \bT \right]}
    {\hat \Sigma_\rd(x)^{1/2}}
    + \sqrt{\frac{n}{\lambda^d}}
    \frac{\E \left[ \hat \mu_\rd(x) \mid \bX, \bT \right] - \mu(x)}
    {\hat \Sigma_\rd(x)^{1/2}} \\
    &=
    \sqrt{\frac{n}{\lambda^d}}
    \frac{\hat \mu_\rd(x) - \E \left[ \hat \mu_\rd(x) \mid \bX, \bT \right]}
    {\hat \Sigma_\rd(x)^{1/2}} \\
    &\quad+
    \sqrt{\frac{n}{\lambda^d}} \,
    O_\P \left(
      \frac{1}{\lambda^\beta}
      + \frac{1}{\lambda \sqrt B}
      + \frac{\log n}{\lambda} \sqrt{\frac{\lambda^d}{n}}
    \right).
  \end{align*}
  %
  The first term now converges weakly to $\cN(0,1)$ by
  Slutsky's theorem, Theorem~\ref{thm:clt_debiased},
  and Theorem~\ref{thm:variance_estimation_debiased},
  while the second term is $o_\P(1)$ by assumption.
  Validity of the confidence interval follows immediately.
  %
\end{proof}

\subsection*{Proof of minimax optimality with debiasing}

\begin{proof}[Theorem~\ref{thm:mondrian_minimax}]

  The bias--variance decomposition along with
  Theorem~\ref{thm:bias_debiased}
  and the proof of Theorem~\ref{thm:clt_debiased}
  with $J = \lfloor \flbeta / 2 \rfloor$ gives
  %
  \begin{align*}
    \E \left[
      \big(
        \hat \mu_\rd(x)
        - \mu(x)
      \big)^2
    \right]
    &=
    \E \left[
      \big(
        \hat \mu_\rd(x)
        - \E \left[ \hat \mu_\rd(x) \mid \bX, \bT \right]
      \big)^2
    \right]
    + \E \left[
      \big(
        \E \left[ \hat \mu_\rd(x) \mid \bX, \bT \right]
        - \mu(x)
      \big)^2
    \right] \\
    &\lesssim
    \frac{\lambda^d}{n}
    + \frac{1}{\lambda^{2\beta}}
    + \frac{1}{\lambda^2 B}.
  \end{align*}
  %
  Note that we used an $L^2$ version of Theorem~\ref{thm:bias_debiased}
  which is immediate from the proof of Theorem~\ref{thm:bias},
  since we obtain the bound in probability through
  Chebyshev's inequality.
  Now since
  $\lambda \asymp n^{\frac{1}{d + 2 \beta}}$
  and
  $B \gtrsim n^{\frac{2 \beta - 2}{d + 2 \beta}}$,
  %
  \begin{align*}
    \E \left[
      \big(
        \hat \mu_\rd(x)
        - \mu(x)
      \big)^2
    \right]
    &\lesssim
    n^{-\frac{2\beta}{d + 2 \beta}}.
  \end{align*}
\end{proof}
