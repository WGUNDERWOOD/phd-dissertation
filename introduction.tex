%! TeX root = dissertation.tex

\chapter{Introduction}

% nonparametric estimation is common
Nonparametric estimation procedures are at the heart of many contemporary
theoretical and methodological topics within the fields of statistics, data
science, and machine learning. Where classical parametric techniques impose
specific distributional and structural assumptions when modeling statistical
problems, nonparametric methods instead take a more flexible approach,
typically positing only higher-level restrictions such as moment conditions,
independence criteria, and smoothness assumptions. Examples of such procedures
abound in modern data science and machine learning, encompassing histograms,
kernel estimators, smoothing splines, decision trees, nearest neighbor methods,
random forests, neural networks, and many more.

% nonparametric estimation is good
The benefits of the nonparametric framework are clear; statistical procedures
can be formulated in cases where the stringent assumptions of parametric models
are untestable, demonstrably violated, or simply unreasonable.
Further, the resulting
methods often as a consequence inherit desirable robustness properties against
various forms of misspecification or misuse. The class of problems which can be
formulated is correspondingly larger: arbitrary distributions and
relationships between variables can be characterized and estimated in a
principled and general manner.

% nonparametric estimation is hard
Nonetheless, these attractive properties do come at a price. In particular, as
its name suggests, the nonparametric approach forgoes the ability to reduce
a complex statistical problem to that of estimating a fixed finite number of
parameters. Rather, nonparametric procedures typically involve making inference
about a growing number of parameters simultaneously, as witnessed in
high-dimensional regimes, or even directly handling infinite-dimensional
objects such as entire regression functions or distributions. As a consequence,
nonparametric estimators are usually less efficient than their corresponding
correctly specified parametric counterparts, when these are available; rates of
convergence tend to be slower, and confidence sets more conservative. Another
challenge is that theoretical mathematical analyses of nonparametric estimators
are often significantly more demanding than those required for low-dimensional
parametric settings, necessitating tools from contemporary developments in
high-dimensional concentration phenomena, coupling and strong approximation
theory, empirical processes, mathematical optimization, and stochastic
calculus.

% nonparametric inference
In addition to providing accurate point estimates of unknown (possibly
high-dimensional or infinite-dimensional) quantities of interest, modern
nonparametric procedures are also expected to come equipped with methodologies
for conducting statistical inference. The availability of such inferential
techniques is paramount, with contemporary nonparametric methods forming a
ubiquitous component of modern data science tool kits; valid uncertainty
quantification is essential for hypothesis testing, error bar construction,
assessing statistical significance, and performing power analyses. Inference is
a central concept in classical statistics, and despite the rapid
recent development of theory for modern nonparametric estimators, their
applicability to statistical inference is in certain cases rather less well
studied: theoretically sound and practically implementable inference procedures
are sometimes absent in the literature.

% complex data
In any statistical modeling problem, the selection and application of an
estimator must naturally be tailored to the available data. Today, much of the
data produced and analyzed does not necessarily fit neatly into the classical
framework of independent and identically distributed samples, and instead might
consist of time series, stochastic processes, networks,
or high-dimensional or functional data, to name just a few.
Therefore it is important to understand how nonparametric methods might be
adapted to correctly handle these data types, maintaining fast estimation rates
and valid techniques for statistical inference. The technical challenges
associated with such an endeavor are non-trivial; many standard techniques are
ineffective in the presence of dependent or infinite-dimensional data for
example. As such, the development of new mathematical results in probability
theory plays an important role in the comprehensive treatment of nonparametric
statistics with complex data.

\section*{Overview of the dissertation}

% what we do
This dissertation presents a selection of topics related to nonparametric
estimation and inference, their applications to complex data, and the technical
mathematical tools used in their theoretical analysis.

% mondrian
Chapter~\ref{ch:mondrian}, titled ``Inference with Mondrian Random Forests,''
is based on the work of \cite{cattaneo2023inference}.
% what are random forests
Random forests are popular ensembling-based methods for classification and
regression, which are well known for their good performance, flexibility,
robustness and efficiency. The majority of random forest models share the
following common framework for producing estimates of a classification or
regression function using covariates and a response variable. Firstly, the
covariate space is partitioned in some algorithmic manner, possibly using a
source of external randomness. Secondly, a local estimator of the
classification or regression function is fitted to the responses in each cell
separately, yielding a tree estimator. Finally, this process is repeated with
many different partitions, and the resulting tree estimators are averaged to
produce a random forest.

% why are there variants
Many different variants of random forests have been proposed in recent years,
typically with the aim of improving their statistical or computational
properties, or simplifying their construction in order to permit a more
detailed theoretical analysis.
% mondrian random forests
One interesting such example is that of the Mondrian random forest, in which
the underlying partitions (or trees) are constructed independently of the data.
Naturally this restriction rules out many classical random forest models which
exhibit a complex and data-dependent partitioning scheme. Instead, trees are
sampled from a canonical stochastic process, known as the Mondrian process,
which endows the resulting tree and forest estimators with various agreeable
features.

% what we do
In this chapter we study the estimation and inference properties of Mondrian
random forests in the nonparametric regression setting. In particular, we
establish a novel central limit theorem for the estimates made by a Mondrian
random forest which, when combined with a characterization of the bias and a
consistent variance estimator, allows one to perform asymptotically valid
statistical inference, such as constructing confidence intervals, on the
unknown regression function. We also provide a debiasing procedure for Mondrian
random forests which allows them to achieve minimax-optimal estimation rates
with H{\"o}lder smooth regression functions, for any smoothness parameter and
in arbitrary dimension, assuming appropriate parameter tuning.

% kernel
Chapter~\ref{ch:kernel}, titled ``Dyadic Kernel Density Estimators,'' is based
on the work of \cite{cattaneo2024uniform}. Network data plays an important role
in statistics, econometrics and many other data science disciplines, providing
a natural framework for modeling relationships between units, be they people,
financial institutions, proteins, or economic entities. Of prominent interest
is the task of performing statistical estimation and inference with data
sampled from the edges of such networks, known as dyadic data. The archetypal
lack of independence between edges in a network renders many classical
statistical tools unsuited for direct application. As such, researchers must
appeal to techniques tailored to dyadic data in order to accurately capture the
complex structure present in the network.

% broad scope
In this chapter we focus on nonparametric estimation and inference with dyadic
data, and in particular we seek methods which are robust in the sense that our
results should hold uniformly across the support of the data. Such uniformity
guarantees allow for statistical inference in a broader range of settings,
including specification testing and distributional counterfactual analysis. We
specifically consider the problem of uniformly estimating a dyadic Lebesgue
density function, focusing on kernel-based estimators taking the form of dyadic
empirical processes.

% main contributions
Our main contributions include the minimax-optimal uniform convergence rate of
the dyadic kernel density estimator, along with strong approximation results
for the associated standardized and Studentized $t$-processes. A consistent
variance estimator enables the construction of valid and feasible uniform
confidence bands for the unknown density function. We showcase the broad
applicability of our results by developing novel counterfactual density
estimation and inference methodology for dyadic data, which can be used for
causal inference and program evaluation.
% why it is difficult
A crucial feature of dyadic distributions is that they may be ``degenerate'' at
certain points in the support of the data, a property making our analysis
somewhat delicate. Nonetheless our methods for uniform inference remain robust
to the potential presence of such points.
% applications
For implementation purposes, we discuss inference procedures based on positive
semi-definite covariance estimators, mean squared error optimal bandwidth
selectors, and robust bias correction techniques. We illustrate the empirical
finite-sample performance of our methods both in simulations and with
real-world trade data, for which we make comparisons between observed and
counterfactual trade distributions in different years. Our technical results
concerning strong approximations and maximal inequalities are of potential
independent interest.

% yurinskii
Finally, Chapter~\ref{ch:yurinskii}, titled ``Yurinskii's Coupling for
Martingales,'' is based on the work of \cite{cattaneo2022yurinskii}.
Yurinskii's coupling is a popular theoretical tool for non-asymptotic
distributional analysis in mathematical statistics and applied probability.
Coupling theory, also known as strong approximation, provides an alternative
framework to the more classical weak convergence approach to statistical
analysis. Rather than merely approximating the distribution of a random
variable, strong approximation techniques construct a sequence of random
variables which are close almost surely or in probability, often with
finite-sample guarantees.

% what is it used for
Coupling allows distributional analysis in settings where weak convergence
fails, including many applications to nonparametric or high-dimensional
statistics; it is a key technical component in the main strong approximation
results of Chapter~\ref{ch:kernel}. The Yurinskii method specifically offers a
Gaussian coupling with an explicit error bound under easily verified
conditions; originally stated in $\ell^2$-norm for sums of independent random
vectors, it has recently been extended both to the $\ell^p$-norm, for $1 \leq p
\leq \infty$, and to vector-valued martingales in $\ell^2$-norm, under some
strong conditions.

% what we do
We present as our main result a Yurinskii coupling for approximate martingales
in $\ell^p$-norm, under substantially weaker conditions than those previously
imposed. Our formulation further allows for the coupling variable to follow a
more general Gaussian mixture distribution, and we provide a novel third-order
coupling method which gives tighter approximations in certain situations. We
specialize our main result to mixingales, martingales, and independent data,
and derive uniform Gaussian mixture strong approximations for martingale
empirical processes. Applications to nonparametric partitioning-based and local
polynomial regression procedures are provided.

% appendices
Supplementary materials for Chapters~\ref{ch:mondrian}, \ref{ch:kernel} and
\ref{ch:yurinskii} are provided in Appendices~\ref{app:mondrian},
\ref{app:kernel} and \ref{app:yurinskii} respectively. These contain detailed
proofs of the main results, additional technical contributions, and further
discussion.
