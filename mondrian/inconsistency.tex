\documentclass{article}
\input{preamble.tex}
\bibliography{refs}

\title{Mondrian Trees Are Not Minimax Optimal in Uniform Norm}
\author{William G.\ Underwood}

\begin{document}

\maketitle







\section{Setup}

Let $(X_i, Y_i)$ be i.i.d.\ samples from a distribution
on $[0,1] \times [0,1]$.
Write $\mu(x) = \E[Y_i \mid X_i = x]$
and $\varepsilon_i = Y_i - \mu(X_i)$.
Suppose that $X_i$ has a continuous density function $f_X(x)$
which is bounded away from zero on $[0,1]$,
and that $\sigma^2(x) = \Var[\varepsilon_i \mid X_i = x]$
is bounded away from zero on $[0,1]$.
For $\lambda>0$ let $T_\lambda$ be a Mondrian tree
(independent of the data)
on $[0,1]$ with lifetime $\lambda$ as defined
by \cite{mourtada2020minimax}.
Denote the cell of $T_\lambda$ containing a point $x$
by $T_\lambda(x)$.
The centered (bias-free) Mondrian tree estimator of $\mu$ is
%
\begin{align*}
  \widehat\mu(x)
  &=
    \frac{\sum_{i=1}^n \varepsilon_i
    \I\big\{ X_i \in T_\lambda(x) \big\}}
    {\sum_{i=1}^n
    \I\big\{ X_i \in T_\lambda(x) \big\}},
\end{align*}
%
with $0/0 = 0$ as usual.





\section{Results}

The main result gives a lower bound of $\lambda^2/n$
on the conditional variance
of the Mondrian tree estimator which is incompatible with the
minimax analysis for integrated squared error performed by
\cite{mourtada2020minimax},
where the corresponding bound is $\lambda/n$.

\begin{lemma}
  %
  Suppose $\lambda \geq 1$.
  %
  \begin{align*}
    \E\left[
    \sup_{x \in [0,1]}
    \widehat\mu(x)^2
    \right]
    &\geq
      \frac{\inf_{x \in [0,1]} \sigma^2(x)}
      {\sup_{x \in [0,1]} f_X(x)}
      \frac{\lambda^2}{8n}
      \gtrsim
      \frac{\lambda^2}{n}.
  \end{align*}
\end{lemma}



\begin{proof}
  Let $x^* = x^*(T_\lambda)$ denote the midpoint of the smallest
  cell (shortest interval) in $T_\lambda$
  and $\bX = (X_1, \ldots, X_n)$.
  %
  \begin{align*}
    \E\left[
    \sup_{x \in [0,1]}
    \widehat\mu(x)^2
    \right]
    &\geq
      \E\left[
      \widehat\mu(x^*)^2
      \right] \\
    &\geq
      \E\left[
      \left(
      \frac{\sum_{i=1}^n \varepsilon_i
      \I\big\{ X_i \in T_\lambda(x^*) \big\}}
      {\sum_{i=1}^n
      \I\big\{ X_i \in T_\lambda(x^*) \big\}}
      \right)^2
      \right] \\
    &=
      \E\left[
      \frac{\I\big\{\sum_{i=1}^n
      \I\big\{ X_i \in T_\lambda(x^*) \big\} > 0 \big\}}
      {\left(\sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x^*) \big\} \right)^2}
      \E\left[
      \left(
      \sum_{i=1}^n \varepsilon_i
      \I\big\{ X_i \in T_\lambda(x^*) \big\}
      \right)^2
      \Bigm| \bX, T_\lambda
      \right]
      \right] \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x) \
      \E\left[
      \frac{\I\big\{\sum_{i=1}^n
      \I\big\{ X_i \in T_\lambda(x^*) \big\} > 0 \big\}}
      {\sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x^*) \big\}}
      \right] \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      \E\left[
      \frac{1}
      {1 \vee \sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x^*) \big\}}
      \right]
      - \P\left(
      \sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x^*) \big\} = 0
      \right)
      \right)
    \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      \E\left[
      \frac{1}
      {\E\left[\sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x^*) \big\}
      \mid T_\lambda \right]}
      \right] \wedge 1
      - \E\left[
      \P\left(
      X_i \notin T_\lambda(x^*)
      \mid T_\lambda
      \right)^n
      \right]
      \right)
      \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      1 \wedge
      \frac{\E\left[1/ |T_\lambda(x^*)| \right]}
      {n \sup_{x \in [0,1]} f_X(x)}
      - \E\left[
      \left(
      1 - \inf_{x \in [0,1]} f_X(x)\, |T_\lambda(x^*)|
      \right)^n
      \right]
      \right) \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      1 \wedge
      \frac{\E\left[1/ |T_\lambda(x^*)| \right]}
      {n \sup_{x \in [0,1]} f_X(x)}
      - \E\left[
      \exp\left(
      - n \inf_{x \in [0,1]} f_X(x)\, |T_\lambda(x^*)|
      \right)
      \right]
      \right) \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      1 \wedge
      \frac{\E\left[1/ |T_\lambda(x^*)| \right]}
      {n \sup_{x \in [0,1]} f_X(x)}
      - \frac{1}
      {1 + n \inf_{x \in [0,1]} f_X(x) /
      \E\left[1/ |T_\lambda(x^*)| \right]}
      \right).
  \end{align*}
  %

  \pagebreak



  It remains to bound the expectation.
  Note that the distribution of
  $|T_\lambda(x^*)|$ conditional on $\# T_\lambda = k$ is
  the same as the distribution of
  $V_k = \min_{i \neq j} |U_i - U_j|$ where
  $U_1, \ldots, U_k$ are i.i.d.\ $\cU[0,1]$.
  Therefore using the facts that
  $\E[V_k] = \frac{1}{(k+1)^2}$
  \cite{pinelis2019order}
  and the median of $\Pois(\lambda)$ is at least $\lambda - 1$,
  %
  \begin{align*}
    \E\left[
    \frac{1} {|T_\lambda(x^*)|}
    \right]
    &=
      \sum_{k=0}^\infty
      \E\left[
      \frac{1}{V_k}
      \right]
      \P(\# T_\lambda = k)
      \geq
      \frac{1}{\E\left[V_{\lfloor\lambda - 1 \rfloor} \right]}
      \P(\# T_\lambda > \lfloor \lambda - 1 \rfloor)
      \geq
      \frac{\lfloor \lambda \rfloor^2}{2}
      \geq
      \frac{\lambda^2}{8}
  \end{align*}
  %
  whenever $\lambda \geq 1$.
  We conclude that
  %
\begin{align*}
    \E\left[
    \sup_{x \in [0,1]}
    \widehat\mu(x)^2
    \right]
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      1 \wedge
      \frac{\lambda^2 / 8}
      {n \sup_{x \in [0,1]} f_X(x)}
      - \frac{1}
      {1 + 8n \inf_{x \in [0,1]} f_X(x) / \lambda^2}
      \right) \\
    &\geq
      \inf_{x \in [0,1]} \sigma^2(x)
      \left(
      1 \wedge
      \frac{\lambda^2 / 8}
      {n \sup_{x \in [0,1]} f_X(x)}
      - 1 \wedge
      \frac{\lambda^2}
      {8n \inf_{x \in [0,1]} f_X(x)}
      \right) \\
  \end{align*}
  %
\end{proof}
%
Note that since we define the estimator to be zero unless
$\sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x) \big\} > 0$,
we need to check that this event occurs with probability
bounded below by a constant.
But since the variance must go to zero for any consistent estimator,
we must have $\lambda^2/n \to 0$ and so
inserting the $1 \wedge \cdots$ term makes no difference.




then this is equivalent to replacing this term by
$1 \vee \sum_{i=1}^n \I\big\{ X_i \in T_\lambda(x) \big\}$
in the denominator,
yielding an overall rate of
$1 \wedge \frac{\lambda^2}{n}$ by Jensen's inequality.
This is still too slow for minimax rates,





\printbibliography

\end{document}